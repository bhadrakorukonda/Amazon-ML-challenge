{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9006034",
   "metadata": {},
   "source": [
    "##  Basic Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432e4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8811f03",
   "metadata": {},
   "source": [
    "##  Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e582999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in sample_test: 100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys, pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path(r\"D:\\amazon ML challenge\")\n",
    "DATASET_DIR  = PROJECT_ROOT / \"data\"\n",
    "SRC_DIR      = PROJECT_ROOT / \"src\"\n",
    "IMAGES_DIR   = PROJECT_ROOT / \"images\"   # <-- images will be saved here\n",
    "IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# make 'src' importable (so you can do 'from src.utils import ...' later if needed)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# load data (must include 'image_link' column)\n",
    "sample_test = pd.read_csv(DATASET_DIR / \"sample_test.csv\")\n",
    "print(\"Rows in sample_test:\", len(sample_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d300cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_root(markers=(\"requirements.txt\", \".git\", \"data\")):\n",
    "    p = Path.cwd()\n",
    "    for _ in range(6):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT = find_root()\n",
    "ART = ROOT / \"artifacts\"\n",
    "ART.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ddc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe threaded downloader (you can paste this here or put it into src/utils.py)\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd, requests, os, re, time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _safe_name(url: str, idx: int) -> str:\n",
    "    path = urlparse(url).path\n",
    "    base = os.path.basename(path) or f\"img_{idx}.jpg\"\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]\", \"_\", base)\n",
    "\n",
    "def _download_one(url: str, out_dir: Path, idx: int, timeout: int = 15, retries: int = 2):\n",
    "    if not isinstance(url, str) or not url.strip():\n",
    "        return False, \"empty\"\n",
    "    fname = _safe_name(url, idx)\n",
    "    dst = out_dir / fname\n",
    "    if dst.exists():\n",
    "        return True, \"exists\"\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=timeout, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with open(dst, \"wb\") as f:\n",
    "                for chunk in r.iter_content(8192):\n",
    "                    if chunk: f.write(chunk)\n",
    "            return True, \"ok\"\n",
    "        except Exception as e:\n",
    "            if attempt < retries:\n",
    "                time.sleep(0.3 * (attempt + 1))\n",
    "            else:\n",
    "                return False, str(e)\n",
    "\n",
    "def download_images(urls, out_dir: str | Path, max_workers: int = 12, timeout: int = 15, retries: int = 2):\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ser = pd.Series(urls).dropna().astype(str).str.strip()\n",
    "    ser = ser[ser.ne(\"\")].reset_index(drop=True)\n",
    "    ok = fail = skip = 0\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_download_one, url, out_dir, i, timeout, retries): i for i, url in ser.items()}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading\"):\n",
    "            success, msg = fut.result()\n",
    "            if success and msg == \"exists\": skip += 1\n",
    "            elif success: ok += 1\n",
    "            else: fail += 1\n",
    "    print(f\"Done. ok={ok}, skipped(existing)={skip}, failed={fail}, saved to {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88cf3c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 100/100 [00:00<00:00, 463.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. ok=12, skipped(existing)=88, failed=0, saved to D:\\amazon ML challenge\\images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_images(sample_test[\"image_link\"], IMAGES_DIR, max_workers=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880817f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapes -> train: (75000, 4) | test: (75000, 3)\n",
      "Columns: ['sample_id', 'catalog_content', 'image_link', 'price'] ...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path(r\"D:\\amazon ML challenge\")\n",
    "DATASET_DIR  = PROJECT_ROOT / \"data\"\n",
    "\n",
    "assert (DATASET_DIR / \"train.csv\").exists(), f\"Missing: {DATASET_DIR/'train.csv'}\"\n",
    "assert (DATASET_DIR / \"test.csv\").exists(),  f\"Missing: {DATASET_DIR/'test.csv'}\"\n",
    "\n",
    "train = pd.read_csv(DATASET_DIR / \"train.csv\")\n",
    "test  = pd.read_csv(DATASET_DIR / \"test.csv\")\n",
    "\n",
    "print(\"Loaded shapes -> train:\", train.shape, \"| test:\", test.shape)\n",
    "print(\"Columns:\", list(train.columns)[:12], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06eca4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  \n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6de0d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75000 entries, 0 to 74999\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   sample_id        75000 non-null  int64  \n",
      " 1   catalog_content  75000 non-null  object \n",
      " 2   image_link       75000 non-null  object \n",
      " 3   price            75000 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1a1e1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 4) (75000, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "PROJECT_ROOT = Path(r\"D:\\amazon ML challenge\")\n",
    "DATASET_DIR  = PROJECT_ROOT / \"data\"\n",
    "ART_DIR      = PROJECT_ROOT / \"artifacts\"\n",
    "ART_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "text_col = \"catalog_content\"\n",
    "target   = \"price\"\n",
    "\n",
    "# (You already have these loaded, but keeping them here makes the cell self-contained)\n",
    "train = pd.read_csv(DATASET_DIR / \"train.csv\")\n",
    "test  = pd.read_csv(DATASET_DIR / \"test.csv\")\n",
    "train[text_col] = train[text_col].fillna(\"\")\n",
    "test[text_col]  = test[text_col].fillna(\"\")\n",
    "print(train.shape, test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9244d7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_hi</th>\n",
       "      <th>analyzer</th>\n",
       "      <th>alpha</th>\n",
       "      <th>cv_mae_mean</th>\n",
       "      <th>cv_mae_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300000</td>\n",
       "      <td>2</td>\n",
       "      <td>word</td>\n",
       "      <td>1.2</td>\n",
       "      <td>13.638834</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200000</td>\n",
       "      <td>2</td>\n",
       "      <td>word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.807010</td>\n",
       "      <td>0.107365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150000</td>\n",
       "      <td>2</td>\n",
       "      <td>word</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.057305</td>\n",
       "      <td>0.090877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300000</td>\n",
       "      <td>5</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.720621</td>\n",
       "      <td>0.164665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_features  ngram_hi analyzer  alpha  cv_mae_mean  cv_mae_std\n",
       "2        300000         2     word    1.2    13.638834    0.111400\n",
       "1        200000         2     word    1.0    13.807010    0.107365\n",
       "0        150000         2     word    0.8    14.057305    0.090877\n",
       "3        300000         5  char_wb    1.0    14.720621    0.164665"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mini hyperparam sweep: TF-IDF + Ridge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import re\n",
    "\n",
    "# Reuse your chosen text source; if you used A/B, set text_col accordingly\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "\n",
    "# Safer cleaner (handles NaNs)\n",
    "def _clean_series(series: pd.Series) -> pd.Series:\n",
    "    series = series.fillna(\"\").astype(str).str.lower()\n",
    "    series = series.str.replace(r\"http\\S+|www\\S+|https\\S+\", \"\", regex=True)\n",
    "    series = series.str.replace(r\"[^a-z0-9 ]+\", \" \", regex=True)\n",
    "    return series.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# Only used if your data isn't pre-cleaned:\n",
    "CLEAN_IN_PIPE = (TEXT_COL == \"catalog_content\")\n",
    "\n",
    "def make_pipe(max_features=200_000, ngram_hi=2, analyzer=\"word\", alpha=1.0):\n",
    "    steps = []\n",
    "    if CLEAN_IN_PIPE:\n",
    "        steps.append((\"clean\", FunctionTransformer(_clean_series, validate=False)))\n",
    "    steps += [\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=(1, ngram_hi),\n",
    "            analyzer=analyzer\n",
    "        )),\n",
    "        (\"ridge\", Ridge(alpha=alpha, random_state=42)),\n",
    "    ]\n",
    "    return Pipeline(steps)\n",
    "\n",
    "param_grid = [\n",
    "    # Word-level (fast, strong baseline)\n",
    "    {\"max_features\": 150_000, \"ngram_hi\": 2, \"analyzer\": \"word\", \"alpha\": 0.8},\n",
    "    {\"max_features\": 200_000, \"ngram_hi\": 2, \"analyzer\": \"word\", \"alpha\": 1.0},\n",
    "    {\"max_features\": 300_000, \"ngram_hi\": 2, \"analyzer\": \"word\", \"alpha\": 1.2},\n",
    "    # Light char-grams often help messy text\n",
    "    {\"max_features\": 300_000, \"ngram_hi\": 5, \"analyzer\": \"char_wb\", \"alpha\": 1.0},\n",
    "]\n",
    "\n",
    "X = train[TEXT_COL]\n",
    "y = train[\"price\"].astype(float)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for cfg in param_grid:\n",
    "    maes = []\n",
    "    for tr_idx, va_idx in cv.split(X):\n",
    "        pipe = make_pipe(**cfg)\n",
    "        pipe.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n",
    "        pred = pipe.predict(X.iloc[va_idx])\n",
    "        maes.append(mean_absolute_error(y.iloc[va_idx], pred))\n",
    "    results.append({**cfg, \"cv_mae_mean\": float(np.mean(maes)), \"cv_mae_std\": float(np.std(maes))})\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"cv_mae_mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49ca3975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/submission_final.csv\n"
     ]
    }
   ],
   "source": [
    "# Final training with best hyperparameters\n",
    "from joblib import dump\n",
    "\n",
    "best_cfg = {\"max_features\": 300_000, \"ngram_hi\": 2, \"analyzer\": \"word\", \"alpha\": 1.2}\n",
    "final_pipe = make_pipe(**best_cfg)\n",
    "\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "final_pipe.fit(train[TEXT_COL], train[\"price\"])\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# create artifacts dir relative to your notebook's working dir\n",
    "Path(\"artifacts\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# now your original lines work\n",
    "from joblib import dump\n",
    "dump(final_pipe, \"artifacts/baseline_tfidf_ridge.pkl\")\n",
    "\n",
    "test_pred = final_pipe.predict(test[TEXT_COL])\n",
    "sub = pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": test_pred})\n",
    "sub.to_csv(\"artifacts/submission_final.csv\", index=False)\n",
    "\n",
    "print(\"Saved: artifacts/submission_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0bdd53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook CWD: d:\\amazon ML challenge\\notebooks\n",
      "Found: ['d:\\\\amazon ML challenge\\\\notebooks\\\\artifacts\\\\submission_final.csv']\n",
      "Resolved ROOT: d:\\amazon ML challenge\n",
      "Saving artifacts to: D:\\amazon ML challenge\\artifacts\n",
      "WROTE: D:\\amazon ML challenge\\artifacts\\submission_final.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# SHOW where the notebook is running\n",
    "print(\"Notebook CWD:\", Path.cwd())\n",
    "\n",
    "# Try to locate any 'submission_final.csv' under the project\n",
    "hits = list(Path.cwd().rglob(\"submission_final.csv\"))\n",
    "print(\"Found:\", [str(p) for p in hits])\n",
    "\n",
    "# Robust project-root detection (looks upward for markers)\n",
    "def find_root(markers=(\"requirements.txt\", \".git\", \"data\")):\n",
    "    p = Path.cwd()\n",
    "    for _ in range(6):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT = find_root()\n",
    "ART = ROOT / \"artifacts\"\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Resolved ROOT:\", ROOT)\n",
    "print(\"Saving artifacts to:\", ART.resolve())\n",
    "\n",
    "# If you don't have test loaded, reload\n",
    "try:\n",
    "    _ = test.head()\n",
    "except NameError:\n",
    "    import pandas as pd\n",
    "    test = pd.read_csv(ROOT / \"data\" / \"test.csv\")\n",
    "\n",
    "# Decide text column\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in test.columns else \"catalog_content\"\n",
    "\n",
    "# Load model and re-save submission to the resolved artifacts path\n",
    "pipe = load(ART / \"baseline_tfidf_ridge.pkl\")  # model you already have\n",
    "pred = pipe.predict(test[TEXT_COL])\n",
    "sub = pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred})\n",
    "out_path = ART / \"submission_final.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"WROTE:\", out_path.resolve(), \"| rows:\", len(sub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51081cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout MAE: 13.96168\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\baseline_tfidf_ridge.pkl\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    train[text_col], train[target],\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=200_000,\n",
    "        ngram_range=(1,2),\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        min_df=2\n",
    "    )),\n",
    "    (\"ridge\", Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "pipe.fit(X_tr, y_tr)\n",
    "pred = pipe.predict(X_va)\n",
    "mae = mean_absolute_error(y_va, pred)\n",
    "print(f\"Holdout MAE: {mae:.5f}\")\n",
    "\n",
    "joblib.dump(pipe, ART_DIR / \"baseline_tfidf_ridge.pkl\")\n",
    "print(\"Saved:\", ART_DIR / \"baseline_tfidf_ridge.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edf81d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MAE per fold: [np.float64(13.96172), np.float64(13.8117), np.float64(13.7539), np.float64(13.63553), np.float64(13.82355)]\n",
      "CV MAE mean: 13.797278751313206 ± 0.10581350201708109\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=200_000,\n",
    "        ngram_range=(1,2),\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        min_df=2\n",
    "    )),\n",
    "    (\"ridge\", Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "cv_scores = -cross_val_score(\n",
    "    cv_pipe,\n",
    "    train[text_col], train[target],\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=kf,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"CV MAE per fold:\", [round(s,5) for s in cv_scores])\n",
    "print(\"CV MAE mean:\", cv_scores.mean(), \"±\", cv_scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5528936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: D:\\amazon ML challenge\\artifacts\\submission_baseline.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>24.083521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>25.098775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>22.979930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>4.334911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>63.435262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id      price\n",
       "0     100179  24.083521\n",
       "1     245611  25.098775\n",
       "2     146263  22.979930\n",
       "3      95658   4.334911\n",
       "4      36806  63.435262"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=200_000,\n",
    "        ngram_range=(1,2),\n",
    "        lowercase=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        min_df=2\n",
    "    )),\n",
    "    (\"ridge\", Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "final_pipe.fit(train[text_col], train[target])\n",
    "joblib.dump(final_pipe, ART_DIR / \"final_tfidf_ridge.pkl\")\n",
    "\n",
    "test_pred = final_pipe.predict(test[text_col])\n",
    "\n",
    "# Build submission (match the sample output schema if needed)\n",
    "sub = pd.DataFrame({\n",
    "    \"sample_id\": test[\"sample_id\"],\n",
    "    \"price\": test_pred\n",
    "})\n",
    "sub_path = ART_DIR / \"submission_baseline.csv\"\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(\"Wrote:\", sub_path)\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74cafc83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m samples = random.sample(os.listdir(IMAGES_DIR), \u001b[32m5\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random, os\n",
    "\n",
    "samples = random.sample(os.listdir(IMAGES_DIR), 5)\n",
    "for s in samples:\n",
    "    img = Image.open(IMAGES_DIR / s)\n",
    "    plt.imshow(img)\n",
    "    plt.title(s)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7669a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils loaded from: D:\\amazon ML challenge\\src\\utils.py\n"
     ]
    }
   ],
   "source": [
    "# Try to import from your project utils; define a minimal fallback if not present\n",
    "try:\n",
    "    from src.utils import download_images\n",
    "    import src.utils as _u\n",
    "    print(\"utils loaded from:\", _u.__file__)\n",
    "except Exception as e:\n",
    "    print(\"Could not import src.utils.download_images:\", e, \"\\nDefining a minimal version here.\")\n",
    "    import re, time, requests, os\n",
    "    from urllib.parse import urlparse\n",
    "    def download_images(urls, out_dir, timeout=15):\n",
    "        out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ok = bad = 0\n",
    "        for i, u in enumerate(urls):\n",
    "            if not isinstance(u, str) or not u.strip():\n",
    "                bad += 1; continue\n",
    "            path = urlparse(u).path\n",
    "            base = os.path.basename(path) or f\"img_{i}.jpg\"\n",
    "            base = re.sub(r\"[^A-Za-z0-9._-]\", \"_\", base)\n",
    "            dst = out_dir / base\n",
    "            if dst.exists(): ok += 1; continue\n",
    "            try:\n",
    "                r = requests.get(u, timeout=timeout, stream=True)\n",
    "                r.raise_for_status()\n",
    "                with open(dst, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(8192):\n",
    "                        if chunk: f.write(chunk)\n",
    "                ok += 1\n",
    "            except Exception:\n",
    "                bad += 1; time.sleep(0.2)\n",
    "        print(f\"Downloaded: {ok}, Failed: {bad}, Saved to: {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3353e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd, requests, os, re, time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _safe_name(url: str, idx: int) -> str:\n",
    "    path = urlparse(url).path\n",
    "    base = os.path.basename(path) or f\"img_{idx}.jpg\"\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]\", \"_\", base)\n",
    "\n",
    "def _download_one(url: str, out_dir: Path, idx: int, timeout: int = 15, retries: int = 2):\n",
    "    if not isinstance(url, str) or not url.strip():\n",
    "        return False, \"empty\"\n",
    "    fname = _safe_name(url, idx)\n",
    "    dst = out_dir / fname\n",
    "    if dst.exists():\n",
    "        return True, \"exists\"\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=timeout, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with open(dst, \"wb\") as f:\n",
    "                for chunk in r.iter_content(8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            return True, \"ok\"\n",
    "        except Exception as e:\n",
    "            if attempt < retries:\n",
    "                time.sleep(0.3 * (attempt + 1))\n",
    "            else:\n",
    "                return False, str(e)\n",
    "\n",
    "def download_images(urls, out_dir: str | Path, max_workers: int = 12, timeout: int = 15, retries: int = 2):\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Clean the input a bit\n",
    "    ser = pd.Series(urls).dropna().astype(str).str.strip()\n",
    "    ser = ser[ser.ne(\"\")].reset_index(drop=True)\n",
    "\n",
    "    ok = fail = skip = 0\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_download_one, url, out_dir, i, timeout, retries): i for i, url in ser.items()}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading\"):\n",
    "            success, msg = fut.result()\n",
    "            if success and msg == \"exists\":\n",
    "                skip += 1\n",
    "            elif success:\n",
    "                ok += 1\n",
    "            else:\n",
    "                fail += 1\n",
    "    print(f\"Done. ok={ok}, skipped(existing)={skip}, failed={fail}, saved to {out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2c0410",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Make sure the column exists\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mimage_link\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43msample_test\u001b[49m.columns, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mimage_link\u001b[39m\u001b[33m'\u001b[39m\u001b[33m column not found. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(sample_test.columns)[:\u001b[32m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m download_images(sample_test[\u001b[33m\"\u001b[39m\u001b[33mimage_link\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m(IMAGES_DIR))\n",
      "\u001b[31mNameError\u001b[39m: name 'sample_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Make sure the column exists\n",
    "assert \"image_link\" in sample_test.columns, f\"'image_link' column not found. Got: {list(sample_test.columns)[:10]}\"\n",
    "download_images(sample_test[\"image_link\"], str(IMAGES_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15cad5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET_FOLDER = '../dataset/'\n",
    "#train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "#test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
    "sample_test = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test.csv'))\n",
    "sample_test_out = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test_out.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae4e0f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'download_images' from 'utils' (d:\\amazon ML challenge\\.venv\\Lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_images\n\u001b[32m      2\u001b[39m download_images(sample_test[\u001b[33m'\u001b[39m\u001b[33mimage_link\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33m../images\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'download_images' from 'utils' (d:\\amazon ML challenge\\.venv\\Lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from utils import download_images\n",
    "download_images(sample_test['image_link'], '../images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e1bbc48",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../images'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../images\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m) > \u001b[32m0\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: '../images'"
     ]
    }
   ],
   "source": [
    "assert len(os.listdir('../images')) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ce2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ../images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
