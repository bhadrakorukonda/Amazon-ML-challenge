{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea42ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal cleaner (only if catalog_content_clean doesn't exist yet)\n",
    "if \"catalog_content_clean\" not in locals() and \"catalog_content_clean\" not in train.columns if 'train' in locals() else [False]:\n",
    "    import re\n",
    "    def clean_text(s) -> str:\n",
    "        s = \"\" if s is None else str(s)\n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", s)\n",
    "        s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s)\n",
    "        return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282e8503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook CWD : d:\\amazon ML challenge\\notebooks\n",
      "Project ROOT : d:\\amazon ML challenge\n",
      "DATA exists? : True | True True\n",
      "ART path     : d:\\amazon ML challenge\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# Robust project-root + data loader for 02_stage2.ipynb\n",
    "\n",
    "import re, gc, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"Walk upward until we find a folder that actually contains data/train.csv (or common markers).\"\"\"\n",
    "    p = Path.cwd()\n",
    "    for _ in range(8):\n",
    "        # Preferred: explicit presence of data files\n",
    "        if (p / \"data\" / \"train.csv\").exists() and (p / \"data\" / \"test.csv\").exists():\n",
    "            return p\n",
    "        # Fallback: repo markers + data dir exists\n",
    "        if any((p / m).exists() for m in (\"requirements.txt\", \".git\", \"HANDOFF.md\")) and (p / \"data\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise FileNotFoundError(\"Could not locate project root containing a 'data/' folder.\")\n",
    "\n",
    "ROOT = find_project_root()\n",
    "DATA = ROOT / \"data\"\n",
    "ART  = ROOT / \"artifacts\"\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Notebook CWD :\", Path.cwd())\n",
    "print(\"Project ROOT :\", ROOT)\n",
    "print(\"DATA exists? :\", DATA.exists(), \"|\", (DATA / \"train.csv\").exists(), (DATA / \"test.csv\").exists())\n",
    "print(\"ART path     :\", ART)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(DATA / \"train.csv\")\n",
    "test  = pd.read_csv(DATA / \"test.csv\")\n",
    "\n",
    "# Choose text column (use cleaned if present, else raw)\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "\n",
    "# Safe string views for modeling\n",
    "X_text_tr = train[TEXT_COL].fillna(\"\").astype(str)\n",
    "X_text_te = test[TEXT_COL].fillna(\"\").astype(str)\n",
    "y = train[\"price\"].astype(float).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7460aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e3b4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_word_meta.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "# === Stage-2B minimal: word TF-IDF + cheap meta features → pred_tm + save CSV ===\n",
    "from pathlib import Path\n",
    "import re, numpy as np, pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Resolve paths and data (works whether you're in notebooks/ or project root)\n",
    "ROOT = globals().get(\"ROOT\", Path.cwd())\n",
    "ROOT = ROOT if isinstance(ROOT, Path) else Path(ROOT)\n",
    "DATA = Path(globals().get(\"DATA\", ROOT / \"data\"))\n",
    "ART  = Path(globals().get(\"ART\",  ROOT / \"artifacts\"))\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data if not already present\n",
    "if \"train\" not in globals() or \"test\" not in globals():\n",
    "    train = pd.read_csv(DATA / \"train.csv\")\n",
    "    test  = pd.read_csv(DATA / \"test.csv\")\n",
    "\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "X_text_tr = train[TEXT_COL].fillna(\"\").astype(str)\n",
    "X_text_te = test[TEXT_COL].fillna(\"\").astype(str)\n",
    "y = train[\"price\"].astype(float).values\n",
    "\n",
    "# ---- meta feature extractor ----\n",
    "def extract_meta(s: pd.Series) -> pd.DataFrame:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "\n",
    "    nums = s.str.findall(r\"(?<![a-zA-Z])(\\d+(?:\\.\\d+)?)\")\n",
    "    num_count = nums.apply(len).astype(float)\n",
    "    max_num = nums.apply(lambda xs: max(map(float, xs)) if xs else np.nan).astype(float)\n",
    "    min_num = nums.apply(lambda xs: min(map(float, xs)) if xs else np.nan).astype(float)\n",
    "\n",
    "    pack = s.str.extract(r\"(?:pack of|pack)\\s*(\\d+)|(\\d+)\\s*-\\s*pack|(\\d+)\\s*pack\", expand=True)\n",
    "    pack_n = pack.apply(lambda row: next((int(x) for x in row if pd.notna(x)), np.nan), axis=1).astype(float)\n",
    "\n",
    "    uw = s.str.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(ml|l|oz|g|kg|lb)\")\n",
    "    def norm_units(pairs):\n",
    "        ml = g = None\n",
    "        for val, unit in pairs:\n",
    "            v = float(val)\n",
    "            if unit == \"ml\": ml = (ml or 0) + v\n",
    "            elif unit == \"l\":  ml = (ml or 0) + v*1000\n",
    "            elif unit == \"oz\": g  = (g  or 0) + v*28.3495\n",
    "            elif unit == \"g\":  g  = (g  or 0) + v\n",
    "            elif unit == \"kg\": g  = (g  or 0) + v*1000\n",
    "            elif unit == \"lb\": g  = (g  or 0) + v*453.592\n",
    "        return pd.Series({\"vol_ml\": ml if ml is not None else np.nan,\n",
    "                          \"wt_g\":  g  if g  is not None else np.nan})\n",
    "    unit_df = uw.apply(norm_units)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"num_count\": num_count,\n",
    "        \"max_num\": max_num,\n",
    "        \"min_num\": min_num,\n",
    "        \"pack_n\":  pack_n,\n",
    "    })\n",
    "    df = pd.concat([df, unit_df], axis=1).fillna(0.0)\n",
    "    return df\n",
    "\n",
    "meta_tr = extract_meta(X_text_tr)\n",
    "meta_te = extract_meta(X_text_te)\n",
    "\n",
    "train_aug = pd.DataFrame({TEXT_COL: X_text_tr})\n",
    "test_aug  = pd.DataFrame({TEXT_COL: X_text_te})\n",
    "for c in meta_tr.columns:\n",
    "    train_aug[c] = meta_tr[c]\n",
    "    test_aug[c]  = meta_te[c]\n",
    "\n",
    "numeric_cols = meta_tr.columns.tolist()\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"tfidf_word\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=300_000), TEXT_COL),\n",
    "    (\"num\", Pipeline([(\"scale\", StandardScaler(with_mean=False))]), numeric_cols),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "model = Ridge(alpha=1.0, random_state=42)\n",
    "pipe_tm = Pipeline([(\"ct\", ct), (\"ridge\", model)])\n",
    "\n",
    "# Fit full + predict\n",
    "pipe_tm.fit(train_aug, y)\n",
    "pred_tm = pipe_tm.predict(test_aug)\n",
    "\n",
    "# Save submission for blending\n",
    "out_tm = ART / \"submission_word_meta.csv\"\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_tm}).to_csv(out_tm, index=False)\n",
    "print(\"Saved:\", out_tm.resolve(), \"| rows:\", len(pred_tm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584923f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_ensemble_v1.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "# Simple average of the two Stage-2 submissions\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(ART)  # reuse from earlier if defined\n",
    "df_wc = pd.read_csv(ART / \"submission_wc.csv\")\n",
    "df_tm = pd.read_csv(ART / \"submission_word_meta.csv\")\n",
    "\n",
    "df = df_wc.merge(df_tm, on=\"sample_id\", suffixes=(\"_wc\", \"_tm\"))\n",
    "df[\"price\"] = 0.5*df[\"price_wc\"] + 0.5*df[\"price_tm\"]\n",
    "out = ART / \"submission_ensemble_v1.csv\"\n",
    "df[[\"sample_id\",\"price\"]].to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve(), \"| rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fc62221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ART = D:\\amazon ML challenge\\artifacts\n",
      "Available submissions: ['submission_ensemble_v1.csv', 'submission_final.csv', 'submission_wc.csv', 'submission_word_meta.csv']\n",
      "-57.24441578355 427.01332224186393 19.36302922582783\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse ART if defined; else resolve it from project root\n",
    "ART = Path(globals().get(\"ART\", Path.cwd() / \"artifacts\"))\n",
    "\n",
    "print(\"ART =\", ART.resolve())\n",
    "print(\"Available submissions:\", [p.name for p in ART.glob(\"submission_*.csv\")])\n",
    "\n",
    "df = pd.read_csv(ART / \"submission_ensemble_v1.csv\")\n",
    "assert len(df)==75000 and df[\"sample_id\"].is_unique and not df[\"price\"].isna().any()\n",
    "print(df[\"price\"].min(), df[\"price\"].max(), df[\"price\"].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aa09661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 147/147 [01:09<00:00,  2.12it/s]\n",
      "Batches: 100%|██████████| 147/147 [01:09<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_ensemble_v2.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ART = Path(globals().get(\"ART\", Path.cwd() / \"artifacts\"))\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "\n",
    "# Encode\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tr_vec = model.encode(train[TEXT_COL].fillna(\"\").tolist(), batch_size=512, show_progress_bar=True, convert_to_numpy=True)\n",
    "te_vec = model.encode(test[TEXT_COL].fillna(\"\").tolist(),  batch_size=512, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Fit + predict\n",
    "ridge_emb = Ridge(alpha=1.0, random_state=42).fit(tr_vec, train[\"price\"].values)\n",
    "pred_emb = ridge_emb.predict(te_vec)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_emb}).to_csv(ART/\"submission_sbert.csv\", index=False)\n",
    "\n",
    "# 3-way blend with existing files\n",
    "df_wc = pd.read_csv(ART/\"submission_wc.csv\")\n",
    "df_tm = pd.read_csv(ART/\"submission_word_meta.csv\")\n",
    "df_sb = pd.read_csv(ART/\"submission_sbert.csv\")\n",
    "\n",
    "m = df_wc.merge(df_tm, on=\"sample_id\", suffixes=(\"_wc\",\"_tm\")).merge(df_sb, on=\"sample_id\")\n",
    "m[\"price\"] = 0.4*m[\"price_wc\"] + 0.4*m[\"price_tm\"] + 0.2*m[\"price\"]\n",
    "out = ART/\"submission_ensemble_v2.csv\"\n",
    "m[[\"sample_id\",\"price\"]].to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve(), \"| rows:\", len(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "361117ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-6):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / denom) * 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8254dc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SMAPE (word+char, log target): 52.18% ± 0.38%\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_log.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse your word+char FeatureUnion pipe from Stage-2A\n",
    "wc_features = pipe_wc.named_steps[\"features\"]  # same vectorizers\n",
    "base = Ridge(alpha=1.2, random_state=42)\n",
    "\n",
    "log_ridge = TransformedTargetRegressor(\n",
    "    regressor=Ridge(alpha=1.2, random_state=42),\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "\n",
    "pipe_wc_log = Pipeline([\n",
    "    (\"features\", wc_features),\n",
    "    (\"ridge\", log_ridge),\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc_log.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    p = pipe_wc_log.predict(X_text_tr.iloc[va]).clip(min=1e-6)\n",
    "    scores.append(smape(y[va], p))\n",
    "print(f\"CV SMAPE (word+char, log target): {np.mean(scores):.2f}% ± {np.std(scores):.2f}%\")\n",
    "\n",
    "# Fit full + save submission\n",
    "pipe_wc_log.fit(X_text_tr, y)\n",
    "pred_wc_log = pipe_wc_log.predict(X_text_te).clip(min=1e-6)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_wc_log}).to_csv(ART/\"submission_wc_log.csv\", index=False)\n",
    "print(\"Saved:\", (ART/\"submission_wc_log.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e6f928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SMAPE (word+char, log target): 52.18% ± 0.38%\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_log.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse your word+char FeatureUnion pipe from Stage-2A\n",
    "wc_features = pipe_wc.named_steps[\"features\"]  # same vectorizers\n",
    "base = Ridge(alpha=1.2, random_state=42)\n",
    "\n",
    "log_ridge = TransformedTargetRegressor(\n",
    "    regressor=Ridge(alpha=1.2, random_state=42),\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "\n",
    "pipe_wc_log = Pipeline([\n",
    "    (\"features\", wc_features),\n",
    "    (\"ridge\", log_ridge),\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc_log.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    p = pipe_wc_log.predict(X_text_tr.iloc[va]).clip(min=1e-6)\n",
    "    scores.append(smape(y[va], p))\n",
    "print(f\"CV SMAPE (word+char, log target): {np.mean(scores):.2f}% ± {np.std(scores):.2f}%\")\n",
    "\n",
    "# Fit full + save submission\n",
    "pipe_wc_log.fit(X_text_tr, y)\n",
    "pred_wc_log = pipe_wc_log.predict(X_text_te).clip(min=1e-6)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_wc_log}).to_csv(ART/\"submission_wc_log.csv\", index=False)\n",
    "print(\"Saved:\", (ART/\"submission_wc_log.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f483312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SMAPE (word+char, Gamma GLM): 64.09% ± 0.47%\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_gamma.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "pipe_wc_gamma = Pipeline([\n",
    "    (\"features\", wc_features),\n",
    "    (\"glm\", TweedieRegressor(power=2.0, link=\"log\", alpha=1e-3, max_iter=3000, tol=1e-6, warm_start=True)),\n",
    "])\n",
    "\n",
    "scores = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc_gamma.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    p = pipe_wc_gamma.predict(X_text_tr.iloc[va]).clip(min=1e-6)\n",
    "    scores.append(smape(y[va], p))\n",
    "print(f\"CV SMAPE (word+char, Gamma GLM): {np.mean(scores):.2f}% ± {np.std(scores):.2f}%\")\n",
    "\n",
    "pipe_wc_gamma.fit(X_text_tr, y)\n",
    "pred_wc_gamma = pipe_wc_gamma.predict(X_text_te).clip(min=1e-6)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_wc_gamma}).to_csv(ART/\"submission_wc_gamma.csv\", index=False)\n",
    "print(\"Saved:\", (ART/\"submission_wc_gamma.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50a67c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_smape_opt.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cands = []\n",
    "for fname in [\"submission_wc.csv\", \"submission_wc_log.csv\", \"submission_wc_gamma.csv\"]:\n",
    "    p = (ART/fname)\n",
    "    if p.exists(): cands.append(pd.read_csv(p).rename(columns={\"price\": fname.replace(\".csv\",\"\")}))\n",
    "\n",
    "m = cands[0]\n",
    "for df in cands[1:]:\n",
    "    m = m.merge(df, on=\"sample_id\")\n",
    "\n",
    "# If both log and gamma exist, try a 60/40 toward the lower CV SMAPE\n",
    "cols = [c for c in m.columns if c!=\"sample_id\"]\n",
    "if \"submission_wc_log\" in cols and \"submission_wc_gamma\" in cols:\n",
    "    m[\"price\"] = 0.6*m[\"submission_wc_log\"] + 0.4*m[\"submission_wc_gamma\"]\n",
    "elif \"submission_wc_log\" in cols:\n",
    "    m[\"price\"] = m[\"submission_wc_log\"]\n",
    "else:\n",
    "    m[\"price\"] = m[cols[0]]\n",
    "\n",
    "out = ART/\"submission_wc_smape_opt.csv\"\n",
    "m[[\"sample_id\",\"price\"]].to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ac3c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bins = np.clip(np.floor(np.log1p(y)), 0, 10).astype(int)\n",
    "# If you want to keep KFold, keep the seed and ensure each fold has distribution checked\n",
    "# Or use StratifiedKFold on bins (regression hack):\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5a85f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE: D:\\amazon ML challenge\\artifacts\\test_out.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "# Make the final, portal-ready file from the log-target model\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "ART, DATA = Path(ART), Path(DATA)\n",
    "\n",
    "df = pd.read_csv(ART/\"submission_wc_log.csv\")[[\"sample_id\",\"price\"]]\n",
    "df[\"price\"] = df[\"price\"].astype(float).clip(lower=1e-6)\n",
    "\n",
    "# align to test.csv order (defensive)\n",
    "test_ids = pd.read_csv(DATA/\"test.csv\")[\"sample_id\"]\n",
    "final = test_ids.to_frame().merge(df, on=\"sample_id\", how=\"left\")\n",
    "assert final[\"price\"].notna().all()\n",
    "\n",
    "final.to_csv(ART/\"test_out.csv\", index=False)\n",
    "print(\"WROTE:\", (ART/\"test_out.csv\").resolve(), \"| rows:\", len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "367f2201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF SMAPE — log: 52.18425790496487 | tm: 66.27008136434968\n",
      "Best OOF SMAPE: 52.18% at w_log=1.00, w_tm=0.00\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_smape_blend.csv\n"
     ]
    }
   ],
   "source": [
    "# === Build OOF predictions for two models, tune weights for SMAPE, blend test preds ===\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "ART, DATA = Path(ART), Path(DATA)\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-6):\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    denom = (np.abs(y_true)+np.abs(y_pred)+eps)/2.0\n",
    "    return float(np.mean(np.abs(y_true-y_pred)/denom)*100.0)\n",
    "\n",
    "# Rebuild the two models (same configs you used)\n",
    "identity = FunctionTransformer(lambda s: s, validate=False)\n",
    "\n",
    "# word+char features (same as Stage-2A)\n",
    "wc_features = Pipeline([\n",
    "    (\"union\", \n",
    "     Pipeline(steps=[(\"id\", identity)])),  # placeholder so we can set params next line\n",
    "])\n",
    "# We'll attach vectorizers directly via FeatureUnion-like manual mapping:\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "wc_features = FeatureUnion([\n",
    "    (\"word\", Pipeline([(\"id\", identity), (\"tfidf\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=350_000, min_df=2))])),\n",
    "    (\"char\", Pipeline([(\"id\", identity), (\"tfidf\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,6), max_features=300_000, min_df=2))]))\n",
    "], n_jobs=1)\n",
    "\n",
    "wc_log = Pipeline([\n",
    "    (\"features\", wc_features),\n",
    "    (\"ridge_log\", TransformedTargetRegressor(\n",
    "        regressor=Ridge(alpha=1.2, random_state=42),\n",
    "        func=np.log1p, inverse_func=np.expm1))\n",
    "])\n",
    "\n",
    "# word TF-IDF + meta (same as Stage-2B)\n",
    "numeric_cols = [\"num_count\",\"max_num\",\"min_num\",\"pack_n\",\"vol_ml\",\"wt_g\"]\n",
    "\n",
    "def extract_meta(s: pd.Series) -> pd.DataFrame:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "    nums = s.str.findall(r\"(?<![a-zA-Z])(\\d+(?:\\.\\d+)?)\")\n",
    "    num_count = nums.apply(len).astype(float)\n",
    "    max_num = nums.apply(lambda xs: max(map(float, xs)) if xs else np.nan).astype(float)\n",
    "    min_num = nums.apply(lambda xs: min(map(float, xs)) if xs else np.nan).astype(float)\n",
    "    pack = s.str.extract(r\"(?:pack of|pack)\\s*(\\d+)|(\\d+)\\s*-\\s*pack|(\\d+)\\s*pack\", expand=True)\n",
    "    pack_n = pack.apply(lambda row: next((int(x) for x in row if pd.notna(x)), np.nan), axis=1).astype(float)\n",
    "    uw = s.str.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(ml|l|oz|g|kg|lb)\")\n",
    "    def norm_units(pairs):\n",
    "        ml = g = None\n",
    "        for val, unit in pairs:\n",
    "            v = float(val)\n",
    "            if unit == \"ml\": ml = (ml or 0) + v\n",
    "            elif unit == \"l\":  ml = (ml or 0) + v*1000\n",
    "            elif unit == \"oz\": g  = (g  or 0) + v*28.3495\n",
    "            elif unit == \"g\":  g  = (g  or 0) + v\n",
    "            elif unit == \"kg\": g  = (g  or 0) + v*1000\n",
    "            elif unit == \"lb\": g  = (g  or 0) + v*453.592\n",
    "        return pd.Series({\"vol_ml\": ml if ml is not None else np.nan,\n",
    "                          \"wt_g\":  g  if g  is not None else np.nan})\n",
    "    unit_df = uw.apply(norm_units)\n",
    "    df = pd.DataFrame({\"num_count\": num_count, \"max_num\": max_num, \"min_num\": min_num, \"pack_n\": pack_n})\n",
    "    return pd.concat([df, unit_df], axis=1).fillna(0.0)\n",
    "\n",
    "train_text = X_text_tr\n",
    "test_text  = X_text_te\n",
    "y_vec = y\n",
    "\n",
    "meta_tr = extract_meta(train_text)\n",
    "meta_te = extract_meta(test_text)\n",
    "\n",
    "train_aug = pd.DataFrame({TEXT_COL: train_text})\n",
    "test_aug  = pd.DataFrame({TEXT_COL: test_text})\n",
    "for c in meta_tr.columns:\n",
    "    train_aug[c] = meta_tr[c]\n",
    "    test_aug[c]  = meta_te[c]\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"tfidf_word\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=300_000), TEXT_COL),\n",
    "    (\"num\", Pipeline([(\"scale\", StandardScaler(with_mean=False))]), numeric_cols),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "word_meta = Pipeline([(\"ct\", ct), (\"ridge\", Ridge(alpha=1.0, random_state=42))])\n",
    "\n",
    "# OOF predictions\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_log = np.zeros(len(train_aug))\n",
    "oof_tm  = np.zeros(len(train_aug))\n",
    "\n",
    "for tr_idx, va_idx in cv.split(train_aug):\n",
    "    wc_log.fit(train_text.iloc[tr_idx], y_vec[tr_idx])\n",
    "    oof_log[va_idx] = wc_log.predict(train_text.iloc[va_idx]).clip(min=1e-6)\n",
    "\n",
    "    word_meta.fit(train_aug.iloc[tr_idx], y_vec[tr_idx])\n",
    "    oof_tm[va_idx] = word_meta.predict(train_aug.iloc[va_idx]).clip(min=1e-6)\n",
    "\n",
    "print(\"OOF SMAPE — log:\", smape(y_vec, oof_log), \"| tm:\", smape(y_vec, oof_tm))\n",
    "\n",
    "# Grid search weights to minimize SMAPE on OOF\n",
    "best_w, best_s = None, 1e9\n",
    "for w in np.linspace(0, 1, 21):  # 0.00 .. 1.00 step 0.05\n",
    "    blend = w*oof_log + (1-w)*oof_tm\n",
    "    s = smape(y_vec, blend)\n",
    "    if s < best_s:\n",
    "        best_s, best_w = s, w\n",
    "print(f\"Best OOF SMAPE: {best_s:.2f}% at w_log={best_w:.2f}, w_tm={1-best_w:.2f}\")\n",
    "\n",
    "# Fit both on full train and blend test with tuned weights\n",
    "wc_log.fit(train_text, y_vec)\n",
    "p_log = wc_log.predict(test_text).clip(min=1e-6)\n",
    "\n",
    "word_meta.fit(train_aug, y_vec)\n",
    "p_tm = word_meta.predict(test_aug).clip(min=1e-6)\n",
    "\n",
    "p_blend = best_w*p_log + (1-best_w)*p_tm\n",
    "\n",
    "# Save SMAPE-optimized blend\n",
    "out = ART/\"submission_wc_smape_blend.csv\"\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": p_blend}).to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9f7a0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE: D:\\amazon ML challenge\\artifacts\\test_out.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ART, DATA = Path(ART), Path(DATA)\n",
    "best = \"submission_wc_smape_blend.csv\"  # or \"submission_wc_log.csv\" if you prefer\n",
    "\n",
    "df = pd.read_csv(ART/best)[[\"sample_id\",\"price\"]]\n",
    "df[\"price\"] = df[\"price\"].astype(float).clip(lower=1e-6)\n",
    "\n",
    "test_ids = pd.read_csv(DATA/\"test.csv\")[\"sample_id\"]\n",
    "final = test_ids.to_frame().merge(df, on=\"sample_id\", how=\"left\")\n",
    "assert final[\"price\"].notna().all()\n",
    "\n",
    "final.to_csv(ART/\"test_out.csv\", index=False)\n",
    "print(\"WROTE:\", (ART/\"test_out.csv\").resolve(), \"| rows:\", len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6e9869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE: D:\\amazon ML challenge\\artifacts\\test_out.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "ART, DATA = Path(ART), Path(DATA)\n",
    "\n",
    "df = pd.read_csv(ART/\"submission_wc_log.csv\")[[\"sample_id\",\"price\"]]\n",
    "df[\"price\"] = df[\"price\"].astype(float).clip(lower=1e-6)\n",
    "\n",
    "test_ids = pd.read_csv(DATA/\"test.csv\")[\"sample_id\"]\n",
    "final = test_ids.to_frame().merge(df, on=\"sample_id\", how=\"left\")\n",
    "assert final[\"price\"].notna().all(), \"Missing predictions\"\n",
    "\n",
    "final.to_csv(ART/\"test_out.csv\", index=False)\n",
    "print(\"WROTE:\", (ART/\"test_out.csv\").resolve(), \"| rows:\", len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25c25d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SMAPE (word+char ridge): 68.12% ± 0.19%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    # Avoid divisions by 0 (when both true and pred are 0)\n",
    "    mask = denom != 0\n",
    "    out = np.zeros_like(denom)\n",
    "    out[mask] = diff[mask] / denom[mask]\n",
    "    return np.mean(out) * 100.0\n",
    "\n",
    "# Example: evaluate smape for the word+char pipeline 'pipe_wc'\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    p = pipe_wc.predict(X_text_tr.iloc[va])\n",
    "    scores.append(smape(y[va], p))\n",
    "print(f\"CV SMAPE (word+char ridge): {np.mean(scores):.2f}% ± {np.std(scores):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ba9fb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 250000, 1.0) 52.20 ± 0.38\n",
      "(350000, 300000, 1.2) 52.18 ± 0.38\n",
      "(400000, 350000, 1.2) 52.14 ± 0.38\n",
      "(300000, 300000, 1.5) 52.28 ± 0.39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52.14107187166095, (400000, 350000, 1.2))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search a few strong configs for SMAPE (fast)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-6):\n",
    "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
    "    denom = (np.abs(y_true)+np.abs(y_pred)+eps)/2.0\n",
    "    return float(np.mean(np.abs(y_true-y_pred)/denom)*100.0)\n",
    "\n",
    "cfgs = [\n",
    "    # (word_max, char_max, alpha)\n",
    "    (300_000, 250_000, 1.0),\n",
    "    (350_000, 300_000, 1.2),\n",
    "    (400_000, 350_000, 1.2),\n",
    "    (300_000, 300_000, 1.5),\n",
    "]\n",
    "best = (9e9, None)\n",
    "\n",
    "for wmax,cmax,alpha in cfgs:\n",
    "    feats = FeatureUnion([\n",
    "        (\"word\", Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=wmax, min_df=2))])),\n",
    "        (\"char\", Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,6), max_features=cmax, min_df=2))]))\n",
    "    ], n_jobs=1)\n",
    "    pipe = Pipeline([\n",
    "        (\"features\", feats),\n",
    "        (\"reg\", TransformedTargetRegressor(\n",
    "            regressor=Ridge(alpha=alpha, random_state=42),\n",
    "            func=np.log1p, inverse_func=np.expm1))\n",
    "    ])\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores=[]\n",
    "    for tr,va in cv.split(X_text_tr):\n",
    "        pipe.fit(X_text_tr.iloc[tr], y[tr])\n",
    "        p = pipe.predict(X_text_tr.iloc[va]).clip(min=1e-6)\n",
    "        scores.append(smape(y[va], p))\n",
    "    m = float(np.mean(scores)); s=float(np.std(scores))\n",
    "    print((wmax,cmax,alpha), f\"{m:.2f} ± {s:.2f}\")\n",
    "    if m < best[0]:\n",
    "        best = (m,(wmax,cmax,alpha))\n",
    "\n",
    "best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33d01f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_log_moe.csv\n"
     ]
    }
   ],
   "source": [
    "# Split training by price bins; train one log-target model per bin; predict all test rows with all, then soft-blend by bin probs\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# bins on log-price\n",
    "ylog = np.log1p(y)\n",
    "cuts = np.quantile(ylog, [0, .2, .4, .6, .8, 1.0])\n",
    "bin_id = np.digitize(ylog, cuts[1:-1], right=False)\n",
    "\n",
    "# train KNN on meta signals to estimate bin probs at test time (no labels for test)\n",
    "def extract_light_meta(s: pd.Series) -> pd.DataFrame:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "    lens = s.str.len()\n",
    "    nums = s.str.count(r\"(?<![a-zA-Z])\\d+(?:\\.\\d+)?\")\n",
    "    upp = s.str.count(r\"\\b[A-Z]{2,}\\b\")\n",
    "    return pd.DataFrame({\"len\":lens, \"nums\":nums, \"upp\":upp}).astype(float)\n",
    "\n",
    "meta_tr_l = extract_light_meta(X_text_tr)\n",
    "meta_te_l = extract_light_meta(X_text_te)\n",
    "\n",
    "# bin classifier proxy using KNN on meta (unsupervised-ish routing)\n",
    "knn = KNeighborsRegressor(n_neighbors=25, weights=\"distance\")\n",
    "knn.fit(meta_tr_l, bin_id.astype(float))\n",
    "bin_pred = knn.predict(meta_te_l)  # continuous; we’ll convert to soft weights\n",
    "# Softmax over distances not perfect; approximate soft assignment:\n",
    "# For stability, build one expert per bin and later weight by proximity to each bin center in ylog space\n",
    "centers = np.array([np.mean(ylog[bin_id==b]) for b in range(5)])\n",
    "\n",
    "# Train 5 experts (same architecture as best log-target)\n",
    "wmax,cmax,alpha = best[1]\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "experts=[]\n",
    "feats_tpl = lambda: FeatureUnion([\n",
    "    (\"word\", Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=wmax, min_df=2))])),\n",
    "    (\"char\", Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,6), max_features=cmax, min_df=2))]))\n",
    "], n_jobs=1)\n",
    "\n",
    "for b in range(5):\n",
    "    mask = (bin_id==b)\n",
    "    reg = TransformedTargetRegressor(Ridge(alpha=alpha, random_state=42), func=np.log1p, inverse_func=np.expm1)\n",
    "    pipe = Pipeline([(\"features\", feats_tpl()), (\"reg\", reg)])\n",
    "    pipe.fit(X_text_tr[mask], y[mask])\n",
    "    experts.append(pipe)\n",
    "\n",
    "# Blend experts by proximity in meta space -> use knn output to map to center weights\n",
    "# Map predicted continuous bin value to weights over 5 centers using RBF-like kernel\n",
    "def soft_weights(x, centers, tau=0.8):\n",
    "    # x ~ predicted bin position (0..4); centers -> 0..4\n",
    "    d = np.abs(centers - x)  # but centers are in ylog; map x to center index scale:\n",
    "    # better: linearly map x in [0,4] to centers’ indices:\n",
    "    idx_pos = np.clip(x, 0, 4)\n",
    "    d = np.abs(np.arange(5) - idx_pos)\n",
    "    w = np.exp(-d/tau)\n",
    "    return w / w.sum()\n",
    "\n",
    "# Predict per expert then soft-blend\n",
    "preds = []\n",
    "for pipe in experts:\n",
    "    preds.append(pipe.predict(X_text_te).clip(min=1e-6))\n",
    "preds = np.vstack(preds)  # [5, N]\n",
    "\n",
    "weights = np.vstack([soft_weights(v, centers) for v in np.clip(bin_pred,0,4)])\n",
    "p_moe = (weights * preds.T).sum(axis=1)\n",
    "\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": p_moe}).to_csv(ART/\"submission_wc_log_moe.csv\", index=False)\n",
    "print(\"Saved:\", (ART/\"submission_wc_log_moe.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdd0f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better meta features directly from text (brand-ish tokens, pack math, normalized units)\n",
    "import re\n",
    "\n",
    "def rich_meta(s: pd.Series) -> pd.DataFrame:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "    # brand-ish: first token before '-' or '|' or '—' or ':' or '('\n",
    "    head = s.str.extract(r\"^\\s*([A-Za-z0-9&'\\./]+)\", expand=False).fillna(\"\")\n",
    "    brand_len = head.str.len()\n",
    "    brand_caps = head.str.contains(r\"[A-Z]\").astype(int)\n",
    "\n",
    "    # quantities\n",
    "    nums = s.str.findall(r\"(?<![A-Za-z])(\\d+(?:\\.\\d+)?)\")\n",
    "    num_count = nums.apply(len).astype(float)\n",
    "    max_num = nums.apply(lambda xs: max(map(float, xs)) if xs else 0.0).astype(float)\n",
    "\n",
    "    # pack math: “pack of K” or “K pack” and multiply if quantities likely per item\n",
    "    pack = s.str.extract(r\"(?:pack of|pack)\\s*(\\d+)|(\\d+)\\s*-\\s*pack|(\\d+)\\s*pack\", expand=True)\n",
    "    pack_n = pack.apply(lambda row: next((int(x) for x in row if pd.notna(x)), 1), axis=1).astype(float)\n",
    "\n",
    "    # units normalize\n",
    "    uw = s.str.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(ml|l|oz|g|kg|lb)\")\n",
    "    def norm(pairs):\n",
    "        ml = g = 0.0\n",
    "        for val,u in pairs:\n",
    "            v=float(val)\n",
    "            if u==\"ml\": ml+=v\n",
    "            elif u==\"l\": ml+=v*1000\n",
    "            elif u==\"oz\": g+=v*28.3495\n",
    "            elif u==\"g\": g+=v\n",
    "            elif u==\"kg\": g+=v*1000\n",
    "            elif u==\"lb\": g+=v*453.592\n",
    "        return pd.Series({\"vol_ml\": ml, \"wt_g\": g})\n",
    "    unit_df = uw.apply(norm)\n",
    "\n",
    "    # total content per pack (proxy)\n",
    "    total_ml = unit_df[\"vol_ml\"]*pack_n\n",
    "    total_g  = unit_df[\"wt_g\"]*pack_n\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"brand_len\":brand_len.astype(float),\n",
    "        \"brand_caps\":brand_caps.astype(float),\n",
    "        \"num_count\":num_count,\n",
    "        \"max_num\":max_num,\n",
    "        \"pack_n\":pack_n,\n",
    "        \"vol_ml\":unit_df[\"vol_ml\"].astype(float),\n",
    "        \"wt_g\": unit_df[\"wt_g\"].astype(float),\n",
    "        \"total_ml\": total_ml.astype(float),\n",
    "        \"total_g\": total_g.astype(float),\n",
    "    }).fillna(0.0)\n",
    "\n",
    "meta_tr2 = rich_meta(X_text_tr)\n",
    "meta_te2 = rich_meta(X_text_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afa0ace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124 NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "assert torch.cuda.is_available(), \"CUDA is not available. (It was earlier—did the venv change?)\"\n",
    "print(torch.__version__, torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ecc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "device = \"cuda\"\n",
    "model = SentenceTransformer(\"clip-ViT-B-32\", device=device)\n",
    "\n",
    "IMG_DIR = Path(\"images_dl\")\n",
    "sample_ids = test[\"sample_id\"].head(512).tolist()\n",
    "imgs = [Image.open(IMG_DIR/f\"{sid}.jpg\").convert(\"RGB\") for sid in sample_ids]\n",
    "\n",
    "t0 = time.time()\n",
    "_ = model.encode(imgs, batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "dt = time.time()-t0\n",
    "print(f\"OK: {len(imgs)/dt:.1f} img/s on GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bd2544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Torch: 2.6.0+cu124\n",
      "IMG_DIR: D:\\amazon ML challenge\\notebooks\\images_dl\n",
      "Found 0/512 image files\n",
      "Example expected path: images_dl\\100179.jpg\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Too few images found in IMG_DIR. Fix IMG_DIR and rerun.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Too few images found in IMG_DIR. Fix IMG_DIR and rerun.\n"
     ]
    }
   ],
   "source": [
    "# GPU + image sanity + timed mini-encode (no guessing)\n",
    "import sys, time\n",
    "from pathlib import Path\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 0) Confirm GPU\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemExit(\"CUDA not available — stop here.\")\n",
    "\n",
    "device = \"cuda\"\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "# 1) Point to the ACTUAL images folder used when you downloaded them\n",
    "#    Change this if you used a different folder (e.g., 'images' or 'data/images')\n",
    "IMG_DIR = Path(\"images_dl\")  # <-- tweak if needed\n",
    "print(\"IMG_DIR:\", IMG_DIR.resolve())\n",
    "\n",
    "# 2) Collect the first 512 test ids and check how many image files exist\n",
    "ids = test[\"sample_id\"].head(512).tolist()\n",
    "paths = [IMG_DIR / f\"{sid}.jpg\" for sid in ids]\n",
    "exist_mask = [p.exists() for p in paths]\n",
    "num_exist = sum(exist_mask)\n",
    "print(f\"Found {num_exist}/512 image files\")\n",
    "\n",
    "# If very few exist, you’re pointing to the wrong folder. Show a couple examples:\n",
    "if num_exist < 32:\n",
    "    print(\"Example expected path:\", paths[0])\n",
    "    raise SystemExit(\"Too few images found in IMG_DIR. Fix IMG_DIR and rerun.\")\n",
    "\n",
    "# 3) Load a small, existing batch (fast) and time it\n",
    "kept = [p for p, ok in zip(paths, exist_mask) if ok][:128]  # 128 is enough to measure\n",
    "imgs = []\n",
    "for p in kept:\n",
    "    try:\n",
    "        imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "    except (UnidentifiedImageError, FileNotFoundError) as e:\n",
    "        # skip bad files\n",
    "        pass\n",
    "\n",
    "print(f\"Encoding {len(imgs)} images on GPU …\", flush=True)\n",
    "\n",
    "model = SentenceTransformer(\"clip-ViT-B-32\", device=device)\n",
    "print(\"ST target device:\", getattr(model, \"_target_device\", None))\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "_ = model.encode(\n",
    "    imgs,\n",
    "    batch_size=64,                # increase to 96/128 if VRAM allows\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "dt = time.perf_counter() - t0\n",
    "print(f\"OK: {len(imgs)/dt:.1f} img/s on GPU (batch=64, n={len(imgs)})\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff80c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: d:\\amazon ML challenge\n",
      "Candidates (path, .jpg count):\n",
      "  d:\\amazon ML challenge\\images_dl 146588\n",
      "  d:\\amazon ML challenge\\images 110\n",
      "\n",
      "Selected IMG_DIR: D:\\amazon ML challenge\\images_dl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Try to infer project root the same way we've been doing\n",
    "def find_root(markers=(\"requirements.txt\", \".git\", \"data\", \"HANDOFF.md\")):\n",
    "    p = Path.cwd()\n",
    "    for _ in range(8):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT = find_root()\n",
    "candidates = [\n",
    "    ROOT/\"images_dl\",\n",
    "    ROOT/\"images\",\n",
    "    ROOT/\"data\"/\"images\",\n",
    "    ROOT/\"notebooks\"/\"images_dl\",\n",
    "    Path(\"images_dl\"),\n",
    "    Path(\"images\"),\n",
    "]\n",
    "\n",
    "def jpg_count(d: Path) -> int:\n",
    "    try:\n",
    "        return sum(1 for _ in d.glob(\"*.jpg\"))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "found = [(d, jpg_count(d)) for d in candidates if d.exists()]\n",
    "found = sorted(found, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"Candidates (path, .jpg count):\")\n",
    "for d,c in found:\n",
    "    print(\" \", d, c)\n",
    "\n",
    "IMG_DIR = None\n",
    "if found and found[0][1] >= 100:  # heuristic: at least 100 jpgs\n",
    "    IMG_DIR = found[0][0]\n",
    "    print(\"\\nSelected IMG_DIR:\", IMG_DIR.resolve())\n",
    "else:\n",
    "    print(\"\\nNo suitable images folder found in common locations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13d33ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 512/512 images in: d:\\amazon ML challenge\\images_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda | ST target device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: 28.7 img/s on GPU (n=128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, UnidentifiedImageError\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch, time\n",
    "\n",
    "assert IMG_DIR is not None and IMG_DIR.exists(), \"Set IMG_DIR to the correct folder path.\"\n",
    "\n",
    "ids = test[\"sample_id\"].head(512).tolist()\n",
    "paths = [IMG_DIR / f\"{sid}.jpg\" for sid in ids]\n",
    "ok = [p.exists() for p in paths]\n",
    "print(f\"Found {sum(ok)}/512 images in:\", IMG_DIR)\n",
    "\n",
    "imgs = []\n",
    "for p in [pp for pp,flag in zip(paths,ok) if flag][:128]:\n",
    "    try:\n",
    "        imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "    except (UnidentifiedImageError, FileNotFoundError):\n",
    "        pass\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"clip-ViT-B-32\", device=device)\n",
    "print(\"Using:\", device, \"| ST target device:\", getattr(model, \"_target_device\", None))\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "_ = model.encode(imgs, batch_size=64, show_progress_bar=True,\n",
    "                 convert_to_numpy=True, normalize_embeddings=True)\n",
    "dt = time.perf_counter() - t0\n",
    "print(f\"OK: {len(imgs)/dt:.1f} img/s on GPU (n={len(imgs)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin-wise calibration for SMAPE\n",
    "bins = np.clip(np.floor(np.log1p(y)), 0, 10).astype(int)\n",
    "pred_oof = o1  # use your best model's OOF (log-tuned)\n",
    "\n",
    "cal = {}\n",
    "for b in np.unique(bins):\n",
    "    mask = (bins==b)\n",
    "    # simple 1D search over scaling factor\n",
    "    best_k, best_s = 1.0, 1e9\n",
    "    for k in np.linspace(0.8, 1.2, 41):\n",
    "        s = smape(y[mask], np.clip(pred_oof[mask]*k, 1e-6, None))\n",
    "        if s < best_s: best_s, best_k = s, k\n",
    "    cal[b] = best_k\n",
    "\n",
    "# apply to test based on predicted bin (use the KNN-lite bin proxy from step 2 or simpler: use text length proxies)\n",
    "test_bins = np.clip(np.floor(np.log1p(np.median(y))), 0, 10).astype(int)  # fallback single bin\n",
    "# If you built 'p_final' above and have no per-row bins, skip or set single multiplier ~1.0\n",
    "\n",
    "# Example applying a single calibrated multiplier:\n",
    "# p_final *= cal.get(test_bins, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: d:\\amazon ML challenge\n",
      "DATA: d:\\amazon ML challenge\\data\n",
      "Loaded train/test: (75000, 4) (75000, 3)\n",
      "Candidate: d:\\amazon ML challenge\\images_dl jpg: 146588\n",
      "Candidate: d:\\amazon ML challenge\\images jpg: 110\n",
      "Candidate: d:\\amazon ML challenge\\data\\images jpg: 0\n",
      "Candidate: d:\\amazon ML challenge\\notebooks\\images_dl jpg: 0\n",
      "Using IMG_DIR: D:\\amazon ML challenge\\images_dl\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU | Torch: 2.6.0+cu124\n",
      "Found 256/256 images for quick test in images_dl\n",
      "Timing encode on 128 imgs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 30.2 img/s (GPU OK)\n"
     ]
    }
   ],
   "source": [
    "# === GPU image embeddings: root->data->images->throughput->cached full encode ===\n",
    "import time, sys, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 0) Root + data loader\n",
    "def find_root(markers=(\"requirements.txt\", \".git\", \"data\", \"HANDOFF.md\")):\n",
    "    p = Path.cwd()\n",
    "    for _ in range(8):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT = find_root()\n",
    "DATA = ROOT / \"data\"\n",
    "ART  = ROOT / \"artifacts\"\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "EMB  = ART / \"emb_cache\"\n",
    "EMB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"DATA:\", DATA)\n",
    "\n",
    "# Load train/test\n",
    "train = pd.read_csv(DATA / \"train.csv\")\n",
    "test  = pd.read_csv(DATA / \"test.csv\")\n",
    "print(\"Loaded train/test:\", train.shape, test.shape)\n",
    "\n",
    "# 1) Locate images folder (pick the one with most .jpg)\n",
    "candidates = [\n",
    "    ROOT/\"images_dl\",\n",
    "    ROOT/\"images\",\n",
    "    ROOT/\"data\"/\"images\",\n",
    "    ROOT/\"notebooks\"/\"images_dl\",\n",
    "]\n",
    "def jpg_count(d: Path) -> int:\n",
    "    return sum(1 for _ in d.glob(\"*.jpg\")) if d.exists() else 0\n",
    "\n",
    "cand_counts = sorted([(d, jpg_count(d)) for d in candidates], key=lambda x:x[1], reverse=True)\n",
    "for d,c in cand_counts:\n",
    "    print(\"Candidate:\", d, \"jpg:\", c)\n",
    "\n",
    "if cand_counts and cand_counts[0][1] > 0:\n",
    "    IMG_DIR = cand_counts[0][0]\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find images. Set IMG_DIR to your downloaded images folder (with many .jpg files).\"\n",
    "    )\n",
    "print(\"Using IMG_DIR:\", IMG_DIR.resolve())\n",
    "\n",
    "# 2) GPU sanity\n",
    "assert torch.cuda.is_available(), \"CUDA not available in this kernel/venv.\"\n",
    "device = \"cuda\"\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0), \"| Torch:\", torch.__version__)\n",
    "\n",
    "# 3) Tiny GPU throughput test (fast)\n",
    "sample_ids = test[\"sample_id\"].head(256).tolist()\n",
    "paths = [IMG_DIR / f\"{sid}.jpg\" for sid in sample_ids]\n",
    "ok = [p.exists() for p in paths]\n",
    "print(f\"Found {sum(ok)}/256 images for quick test in {IMG_DIR.name}\")\n",
    "\n",
    "imgs = []\n",
    "for p in [pp for pp,flag in zip(paths,ok) if flag][:128]:\n",
    "    try:\n",
    "        imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "    except (UnidentifiedImageError, FileNotFoundError):\n",
    "        pass\n",
    "print(\"Timing encode on\", len(imgs), \"imgs ...\")\n",
    "\n",
    "model = SentenceTransformer(\"clip-ViT-B-32\", device=device)\n",
    "t0 = time.perf_counter()\n",
    "_ = model.encode(\n",
    "    imgs,\n",
    "    batch_size=64,  # increase to 96/128 if VRAM allows\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "dt = time.perf_counter()-t0\n",
    "if len(imgs) > 0:\n",
    "    print(f\"Throughput: {len(imgs)/dt:.1f} img/s (GPU OK)\")\n",
    "\n",
    "# 4) Full encode with caching (runs once; reuses .npy afterwards)\n",
    "def load_imgs(ids):\n",
    "    out = []\n",
    "    blank = Image.new(\"RGB\", (224,224), color=0)\n",
    "    for sid in ids:\n",
    "        p = IMG_DIR / f\"{sid}.jpg\"\n",
    "        try:\n",
    "            out.append(Image.open(p).convert(\"RGB\"))\n",
    "        except Exception:\n",
    "            out.append(blank)\n",
    "    return out\n",
    "\n",
    "def encode_images(ids, save_path, batch_size=96):\n",
    "    save_path = Path(save_path)\n",
    "    if save_path.exists():\n",
    "        arr = np.load(save_path, mmap_mode=\"r\")\n",
    "        if arr.shape[0] == len(ids):\n",
    "            print(\"Loaded cached:\", save_path)\n",
    "            return np.array(arr)\n",
    "        print(\"Cache size mismatch; recomputing:\", save_path)\n",
    "    imgs_all = load_imgs(ids)\n",
    "    t0 = time.perf_counter()\n",
    "    vec = model.encode(\n",
    "        imgs_all, batch_size=batch_size, show_progress_bar=True,\n",
    "        convert_to_numpy=True, normalize_embeddings=True\n",
    "    )\n",
    "    print(f\"Full encode throughput: {len(imgs_all)/(time.perf_counter()-t0):.1f} img/s\")\n",
    "    np.save(save_path, vec)\n",
    "    print(\"Saved:\", save_path, vec.shape)\n",
    "    return vec\n",
    "\n",
    "tr_ids = train[\"sample_id\"].tolist()\n",
    "te_ids = test[\"sample_id\"].tolist()\n",
    "\n",
    "tr_img_vec = encode_images(tr_ids, EMB/\"train_clip_img.npy\", batch_size=96)\n",
    "te_img_vec = encode_images(te_ids, EMB/\"test_clip_img.npy\",  batch_size=96)\n",
    "print(\"Done. train/test img emb shapes:\", tr_img_vec.shape, te_img_vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ec692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\amazon ML challenge\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU | Torch: 2.6.0+cu124\n",
      "IDs loaded: train=75000, test=75000\n",
      "Model ready: ViT-B-32 laion2b_s34b_b79k\n",
      "[start] train_clip_openclip.npy | total=75000 | bs=256 | workers=6\n"
     ]
    }
   ],
   "source": [
    "# === Encode CLIP image embeddings with live progress + resume ===\n",
    "import time, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import open_clip\n",
    "\n",
    "# ---- CONFIG (edit only these if needed) ----\n",
    "ROOT = Path(r\"D:\\amazon ML challenge\")\n",
    "DATA = ROOT / \"data\"\n",
    "IMG_DIR = Path(r\"D:\\amazon ML challenge\\images_dl\")\n",
    "ART  = ROOT / \"artifacts\"; ART.mkdir(parents=True, exist_ok=True)\n",
    "EMB  = ART / \"emb_cache\";  EMB.mkdir(parents=True, exist_ok=True)\n",
    "BATCH_SIZE   = 256\n",
    "NUM_WORKERS  = 6\n",
    "CHUNK_SIZE   = 10_000  # write partial every 10k rows\n",
    "MODEL_NAME   = \"ViT-B-32\"\n",
    "PRETRAINED   = \"laion2b_s34b_b79k\"\n",
    "# -------------------------------------------\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available in this kernel/venv.\"\n",
    "device = \"cuda\"\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0), \"| Torch:\", torch.__version__, flush=True)\n",
    "\n",
    "# Load ids\n",
    "train = pd.read_csv(DATA / \"train.csv\")\n",
    "test  = pd.read_csv(DATA / \"test.csv\")\n",
    "tr_ids = train[\"sample_id\"].tolist()\n",
    "te_ids = test[\"sample_id\"].tolist()\n",
    "print(f\"IDs loaded: train={len(tr_ids)}, test={len(te_ids)}\", flush=True)\n",
    "\n",
    "# Model + preprocess (fp16)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED, device=device)\n",
    "model.eval()\n",
    "model = model.to(dtype=torch.float16)\n",
    "print(\"Model ready:\", MODEL_NAME, PRETRAINED, flush=True)\n",
    "\n",
    "# Dataset\n",
    "class ImgDS(Dataset):\n",
    "    def __init__(self, ids, img_dir, preprocess, start=0):\n",
    "        self.ids = ids[start:]\n",
    "        self.dir = Path(img_dir)\n",
    "        self.preprocess = preprocess\n",
    "        self.blank = Image.new(\"RGB\", (224,224), color=0)\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        sid = self.ids[i]\n",
    "        p = self.dir / f\"{sid}.jpg\"\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = self.blank\n",
    "        return self.preprocess(img), sid\n",
    "\n",
    "def encode_split_progress(ids, save_path, model, preprocess, img_dir,\n",
    "                          batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, chunk_size=CHUNK_SIZE):\n",
    "    save_path = Path(save_path)\n",
    "    tmp_path  = save_path.with_suffix(\".partial.npy\")\n",
    "\n",
    "    # Resume if partial exists\n",
    "    start_idx = 0\n",
    "    partial = None\n",
    "    if tmp_path.exists():\n",
    "        partial = np.load(tmp_path, mmap_mode=\"r\")\n",
    "        start_idx = partial.shape[0]\n",
    "        print(f\"[resume] {tmp_path.name}: {start_idx}/{len(ids)} rows\", flush=True)\n",
    "\n",
    "    # If full exists and matches length, return it\n",
    "    if save_path.exists():\n",
    "        arr = np.load(save_path, mmap_mode=\"r\")\n",
    "        if arr.shape[0] == len(ids):\n",
    "            print(f\"[cache] Loaded {save_path.name}: {arr.shape}\", flush=True)\n",
    "            return np.array(arr)\n",
    "\n",
    "    ds = ImgDS(ids, img_dir, preprocess, start=start_idx)\n",
    "    if len(ds) == 0:\n",
    "        if partial is not None:\n",
    "            np.save(save_path, np.array(partial))\n",
    "            tmp_path.unlink(missing_ok=True)\n",
    "            print(f\"[finalize] Saved {save_path.name} {partial.shape}\", flush=True)\n",
    "            return np.array(partial)\n",
    "        else:\n",
    "            raise RuntimeError(\"No samples to encode.\")\n",
    "\n",
    "    dl = DataLoader(\n",
    "        ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    embs = []\n",
    "    processed = start_idx\n",
    "    t0 = time.perf_counter()\n",
    "    last = t0\n",
    "    print(f\"[start] {save_path.name} | total={len(ids)} | bs={batch_size} | workers={num_workers}\", flush=True)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for bi, (xb, _) in enumerate(dl, 1):\n",
    "            xb = xb.to(device, non_blocking=True).to(dtype=torch.float16)\n",
    "            z  = model.encode_image(xb)\n",
    "            z  = torch.nn.functional.normalize(z.float(), dim=1)\n",
    "            embs.append(z.cpu())\n",
    "            processed += xb.size(0)\n",
    "\n",
    "            # progress line every ~5 batches\n",
    "            if bi % 5 == 0:\n",
    "                now = time.perf_counter()\n",
    "                inst = (xb.size(0)*5) / (now - last)\n",
    "                overall = processed / (now - t0)\n",
    "                last = now\n",
    "                print(f\"[{processed:6d}/{len(ids)}] ~{int(overall)} img/s (inst ~{int(inst)})\", flush=True)\n",
    "\n",
    "            # periodic checkpoint\n",
    "            if processed % chunk_size == 0:\n",
    "                E_chunk = torch.cat(embs).numpy()\n",
    "                if partial is not None:\n",
    "                    E_chunk = np.vstack([partial, E_chunk])\n",
    "                np.save(tmp_path, E_chunk)\n",
    "                print(f\"[ckpt] {tmp_path.name} -> {E_chunk.shape}\", flush=True)\n",
    "                embs, partial = [], E_chunk  # reset buffer\n",
    "\n",
    "    # finalize\n",
    "    E = torch.cat(embs).numpy() if embs else np.empty((0, 512), np.float32)\n",
    "    if partial is not None:\n",
    "        E = np.vstack([partial, E])\n",
    "    np.save(save_path, E)\n",
    "    tmp_path.unlink(missing_ok=True)\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"[done] {save_path.name} {E.shape} | ~{int(len(ids)/dt)} img/s\", flush=True)\n",
    "    return E\n",
    "\n",
    "# ---- CALL THE FUNCTION (this actually runs it) ----\n",
    "train_out = EMB / \"train_clip_openclip.npy\"\n",
    "test_out  = EMB / \"test_clip_openclip.npy\"\n",
    "\n",
    "tr_img_vec = encode_split_progress(tr_ids, train_out, model, preprocess, IMG_DIR)\n",
    "te_img_vec = encode_split_progress(te_ids,  test_out,  model, preprocess, IMG_DIR)\n",
    "\n",
    "print(\"Embeddings ready:\", tr_img_vec.shape, te_img_vec.shape, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239017e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\amazon ML challenge\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU | torch: 2.6.0+cu124\n",
      "IDs: 75000 75000\n"
     ]
    }
   ],
   "source": [
    "# Rebuild dataset + model (safe on Windows)\n",
    "import torch, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import open_clip\n",
    "\n",
    "ROOT = Path(r\"D:\\amazon ML challenge\")\n",
    "DATA = ROOT / \"data\"\n",
    "IMG_DIR = Path(r\"D:\\amazon ML challenge\\images_dl\")\n",
    "ART = ROOT / \"artifacts\"; ART.mkdir(parents=True, exist_ok=True)\n",
    "EMB = ART / \"emb_cache\"; EMB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available\"\n",
    "device = \"cuda\"\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0), \"| torch:\", torch.__version__)\n",
    "\n",
    "train = pd.read_csv(DATA/\"train.csv\"); test = pd.read_csv(DATA/\"test.csv\")\n",
    "tr_ids = train[\"sample_id\"].tolist(); te_ids = test[\"sample_id\"].tolist()\n",
    "print(\"IDs:\", len(tr_ids), len(te_ids))\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\", device=device\n",
    ")\n",
    "model.eval(); model = model.to(dtype=torch.float16)\n",
    "\n",
    "class ImgDS(Dataset):\n",
    "    def __init__(self, ids, img_dir, preprocess, start=0):\n",
    "        self.ids = ids[start:]; self.dir = Path(img_dir)\n",
    "        self.preprocess = preprocess; self.blank = Image.new(\"RGB\",(224,224),color=0)\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        sid = self.ids[i]; p = self.dir / f\"{sid}.jpg\"\n",
    "        try: img = Image.open(p).convert(\"RGB\")\n",
    "        except Exception: img = self.blank\n",
    "        return self.preprocess(img), sid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b459b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded batch: torch.Size([64, 3, 224, 224]) in 1.83 s\n",
      "Encoded OK, emb: torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "ds_dbg = ImgDS(tr_ids, IMG_DIR, preprocess, start=0)\n",
    "dl_dbg = DataLoader(ds_dbg, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "import time\n",
    "t0 = time.perf_counter()\n",
    "xb, _ = next(iter(dl_dbg))\n",
    "print(\"Loaded batch:\", xb.shape, \"in\", round(time.perf_counter()-t0,2), \"s\")\n",
    "xb = xb.to(\"cuda\", non_blocking=True).to(dtype=torch.float16)\n",
    "with torch.inference_mode(): z = model.encode_image(xb)\n",
    "print(\"Encoded OK, emb:\", z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6018ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU | Torch: 2.6.0+cu124\n",
      "[start] train_clip_openclip.npy total=75000 bs=256 workers=0\n",
      "[  1280/75000] ~43 img/s (inst ~43)\n",
      "[  2560/75000] ~41 img/s (inst ~39)\n",
      "[  3840/75000] ~39 img/s (inst ~36)\n",
      "[  5120/75000] ~38 img/s (inst ~34)\n",
      "[  6400/75000] ~36 img/s (inst ~32)\n",
      "[  7680/75000] ~34 img/s (inst ~27)\n",
      "[  8960/75000] ~33 img/s (inst ~28)\n",
      "[ 10240/75000] ~32 img/s (inst ~27)\n",
      "[ 11520/75000] ~32 img/s (inst ~27)\n",
      "[ 12800/75000] ~31 img/s (inst ~28)\n",
      "[ 14080/75000] ~31 img/s (inst ~27)\n",
      "[ 15360/75000] ~30 img/s (inst ~26)\n",
      "[ 16640/75000] ~30 img/s (inst ~24)\n",
      "[ 17920/75000] ~29 img/s (inst ~24)\n",
      "[ 19200/75000] ~29 img/s (inst ~25)\n",
      "[ 20480/75000] ~28 img/s (inst ~24)\n",
      "[ 21760/75000] ~28 img/s (inst ~24)\n",
      "[ 23040/75000] ~28 img/s (inst ~24)\n",
      "[ 24320/75000] ~28 img/s (inst ~24)\n",
      "[ 25600/75000] ~28 img/s (inst ~24)\n",
      "[ 26880/75000] ~27 img/s (inst ~24)\n",
      "[ 28160/75000] ~27 img/s (inst ~24)\n",
      "[ 29440/75000] ~27 img/s (inst ~24)\n",
      "[ 30720/75000] ~27 img/s (inst ~25)\n",
      "[ 32000/75000] ~27 img/s (inst ~25)\n",
      "[ckpt] train_clip_openclip.partial.npy -> (32000, 512)\n",
      "[ 33280/75000] ~27 img/s (inst ~50)\n",
      "[ 34560/75000] ~28 img/s (inst ~751)\n",
      "[ 35840/75000] ~29 img/s (inst ~764)\n",
      "[ 37120/75000] ~29 img/s (inst ~27)\n",
      "[ 38400/75000] ~29 img/s (inst ~25)\n",
      "[ 39680/75000] ~29 img/s (inst ~24)\n",
      "[ 40960/75000] ~29 img/s (inst ~25)\n",
      "[ 42240/75000] ~29 img/s (inst ~26)\n",
      "[ 43520/75000] ~29 img/s (inst ~25)\n",
      "[ 44800/75000] ~28 img/s (inst ~25)\n",
      "[ 46080/75000] ~28 img/s (inst ~25)\n",
      "[ 47360/75000] ~28 img/s (inst ~25)\n",
      "[ 48640/75000] ~28 img/s (inst ~25)\n",
      "[ 49920/75000] ~28 img/s (inst ~24)\n",
      "[ 51200/75000] ~28 img/s (inst ~24)\n",
      "[ 52480/75000] ~28 img/s (inst ~24)\n",
      "[ 53760/75000] ~28 img/s (inst ~24)\n",
      "[ 55040/75000] ~28 img/s (inst ~25)\n",
      "[ 56320/75000] ~28 img/s (inst ~24)\n",
      "[ 57600/75000] ~27 img/s (inst ~25)\n",
      "[ 58880/75000] ~27 img/s (inst ~25)\n",
      "[ 60160/75000] ~27 img/s (inst ~25)\n",
      "[ 61440/75000] ~27 img/s (inst ~24)\n",
      "[ 62720/75000] ~27 img/s (inst ~24)\n",
      "[ 64000/75000] ~27 img/s (inst ~26)\n",
      "[ckpt] train_clip_openclip.partial.npy -> (64000, 512)\n",
      "[ 65280/75000] ~27 img/s (inst ~26)\n",
      "[ 66560/75000] ~27 img/s (inst ~25)\n",
      "[ 67840/75000] ~27 img/s (inst ~24)\n",
      "[ 69120/75000] ~27 img/s (inst ~24)\n",
      "[ 70400/75000] ~27 img/s (inst ~25)\n",
      "[ 71680/75000] ~27 img/s (inst ~26)\n",
      "[ 72960/75000] ~27 img/s (inst ~26)\n",
      "[ 74240/75000] ~27 img/s (inst ~25)\n",
      "[done] train_clip_openclip.npy (75000, 512) ~27 img/s\n",
      "[start] test_clip_openclip.npy total=75000 bs=256 workers=0\n",
      "[  1280/75000] ~26 img/s (inst ~26)\n",
      "[  2560/75000] ~25 img/s (inst ~24)\n",
      "[  3840/75000] ~25 img/s (inst ~24)\n",
      "[  5120/75000] ~25 img/s (inst ~25)\n",
      "[  6400/75000] ~25 img/s (inst ~25)\n",
      "[  7680/75000] ~25 img/s (inst ~24)\n",
      "[  8960/75000] ~25 img/s (inst ~24)\n",
      "[ 10240/75000] ~24 img/s (inst ~24)\n",
      "[ 11520/75000] ~24 img/s (inst ~25)\n",
      "[ 12800/75000] ~25 img/s (inst ~25)\n",
      "[ 14080/75000] ~25 img/s (inst ~24)\n",
      "[ 15360/75000] ~25 img/s (inst ~24)\n",
      "[ 16640/75000] ~25 img/s (inst ~25)\n",
      "[ 17920/75000] ~25 img/s (inst ~25)\n",
      "[ 19200/75000] ~24 img/s (inst ~24)\n",
      "[ 20480/75000] ~24 img/s (inst ~25)\n",
      "[ 21760/75000] ~24 img/s (inst ~24)\n",
      "[ 23040/75000] ~24 img/s (inst ~24)\n",
      "[ 24320/75000] ~24 img/s (inst ~25)\n",
      "[ 25600/75000] ~24 img/s (inst ~24)\n",
      "[ 26880/75000] ~24 img/s (inst ~25)\n",
      "[ 28160/75000] ~24 img/s (inst ~25)\n",
      "[ 29440/75000] ~24 img/s (inst ~24)\n",
      "[ 30720/75000] ~24 img/s (inst ~26)\n",
      "[ 32000/75000] ~24 img/s (inst ~25)\n",
      "[ckpt] test_clip_openclip.partial.npy -> (32000, 512)\n",
      "[ 33280/75000] ~24 img/s (inst ~25)\n",
      "[ 34560/75000] ~24 img/s (inst ~25)\n",
      "[ 35840/75000] ~24 img/s (inst ~24)\n",
      "[ 37120/75000] ~24 img/s (inst ~25)\n",
      "[ 38400/75000] ~24 img/s (inst ~25)\n",
      "[ 39680/75000] ~25 img/s (inst ~25)\n",
      "[ 40960/75000] ~25 img/s (inst ~25)\n",
      "[ 42240/75000] ~25 img/s (inst ~25)\n",
      "[ 43520/75000] ~25 img/s (inst ~25)\n",
      "[ 44800/75000] ~25 img/s (inst ~25)\n",
      "[ 46080/75000] ~25 img/s (inst ~25)\n",
      "[ 47360/75000] ~25 img/s (inst ~24)\n",
      "[ 48640/75000] ~25 img/s (inst ~25)\n",
      "[ 49920/75000] ~25 img/s (inst ~24)\n",
      "[ 51200/75000] ~25 img/s (inst ~24)\n",
      "[ 52480/75000] ~25 img/s (inst ~25)\n",
      "[ 53760/75000] ~25 img/s (inst ~25)\n",
      "[ 55040/75000] ~25 img/s (inst ~24)\n",
      "[ 56320/75000] ~25 img/s (inst ~25)\n",
      "[ 57600/75000] ~25 img/s (inst ~26)\n",
      "[ 58880/75000] ~25 img/s (inst ~26)\n",
      "[ 60160/75000] ~25 img/s (inst ~25)\n",
      "[ 61440/75000] ~24 img/s (inst ~15)\n",
      "[ 62720/75000] ~24 img/s (inst ~25)\n",
      "[ 64000/75000] ~24 img/s (inst ~24)\n",
      "[ckpt] test_clip_openclip.partial.npy -> (64000, 512)\n",
      "[ 65280/75000] ~24 img/s (inst ~24)\n",
      "[ 66560/75000] ~24 img/s (inst ~25)\n",
      "[ 67840/75000] ~24 img/s (inst ~25)\n",
      "[ 69120/75000] ~24 img/s (inst ~25)\n",
      "[ 70400/75000] ~24 img/s (inst ~25)\n",
      "[ 71680/75000] ~24 img/s (inst ~24)\n",
      "[ 72960/75000] ~24 img/s (inst ~25)\n",
      "[ 74240/75000] ~24 img/s (inst ~25)\n",
      "[done] test_clip_openclip.npy (75000, 512) ~24 img/s\n",
      "Embeddings: (75000, 512) (75000, 512)\n"
     ]
    }
   ],
   "source": [
    "# === Reliable OpenCLIP encoder (Windows-safe, resumable, frequent checkpoints) ===\n",
    "import time, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import open_clip\n",
    "\n",
    "# ---- Paths (your project) ----\n",
    "ROOT = Path(r\"D:\\amazon ML challenge\")\n",
    "DATA = ROOT / \"data\"\n",
    "IMG_DIR = Path(r\"D:\\amazon ML challenge\\images_dl\")\n",
    "ART  = ROOT / \"artifacts\"; ART.mkdir(parents=True, exist_ok=True)\n",
    "EMB  = ART / \"emb_cache\";  EMB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Load data ----\n",
    "train = pd.read_csv(DATA/\"train.csv\"); test  = pd.read_csv(DATA/\"test.csv\")\n",
    "tr_ids = train[\"sample_id\"].tolist(); te_ids = test[\"sample_id\"].tolist()\n",
    "\n",
    "# ---- GPU + model ----\n",
    "assert torch.cuda.is_available(), \"CUDA not available in this kernel/venv.\"\n",
    "device = \"cuda\"; torch.backends.cudnn.benchmark = True\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\", device=device)\n",
    "model.eval(); model = model.to(dtype=torch.float16)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0), \"| Torch:\", torch.__version__)\n",
    "\n",
    "# ---- Dataset ----\n",
    "class ImgDS(Dataset):\n",
    "    def __init__(self, ids, img_dir, preprocess, start=0):\n",
    "        self.ids = ids[start:]; self.dir = Path(img_dir)\n",
    "        self.preprocess = preprocess; self.blank = Image.new(\"RGB\",(224,224),color=0)\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        sid = self.ids[i]; p = self.dir / f\"{sid}.jpg\"\n",
    "        try: img = Image.open(p).convert(\"RGB\")\n",
    "        except Exception: img = self.blank\n",
    "        return self.preprocess(img), sid\n",
    "\n",
    "# ---- Resumable, frequent-checkpoint encoder (workers=0 to avoid Windows hangs) ----\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def encode_split_solo(ids, out_path, batch_size=256, chunk=2_000):\n",
    "    out_path = Path(out_path); tmp = out_path.with_suffix(\".partial.npy\")\n",
    "\n",
    "    # resume if partial exists\n",
    "    start = 0; part = None\n",
    "    if tmp.exists():\n",
    "        part = np.load(tmp, mmap_mode=\"r\"); start = part.shape[0]\n",
    "        print(f\"[resume] {tmp.name}: {start}/{len(ids)}\")\n",
    "\n",
    "    # if full exists, return\n",
    "    if out_path.exists():\n",
    "        arr = np.load(out_path, mmap_mode=\"r\")\n",
    "        if arr.shape[0] == len(ids):\n",
    "            print(f\"[cache] {out_path.name}: {arr.shape}\"); return np.array(arr)\n",
    "\n",
    "    ds = ImgDS(ids, IMG_DIR, preprocess, start=start)\n",
    "    if len(ds)==0:\n",
    "        if part is not None:\n",
    "            np.save(out_path, np.array(part)); tmp.unlink(missing_ok=True)\n",
    "            print(f\"[finalize] {out_path.name} {part.shape}\"); return np.array(part)\n",
    "        raise RuntimeError(\"No samples to encode.\")\n",
    "\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    embs=[]; done=start; t0=time.perf_counter(); last=t0\n",
    "    print(f\"[start] {out_path.name} total={len(ids)} bs={batch_size} workers=0\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i,(xb,_) in enumerate(dl,1):\n",
    "            xb = xb.to(\"cuda\", non_blocking=True).to(dtype=torch.float16)\n",
    "            z = model.encode_image(xb)\n",
    "            z = torch.nn.functional.normalize(z.float(), dim=1)\n",
    "            embs.append(z.cpu()); done += xb.size(0)\n",
    "\n",
    "            if i%5==0:\n",
    "                now=time.perf_counter()\n",
    "                inst = (xb.size(0)*5)/(now-last); overall = done/(now-t0); last=now\n",
    "                print(f\"[{done:6d}/{len(ids)}] ~{int(overall)} img/s (inst ~{int(inst)})\", flush=True)\n",
    "\n",
    "            if done%chunk==0:\n",
    "                E = torch.cat(embs).numpy()\n",
    "                if part is not None: E = np.vstack([part,E])\n",
    "                np.save(tmp, E); print(f\"[ckpt] {tmp.name} -> {E.shape}\", flush=True)\n",
    "                embs=[]; part=E\n",
    "\n",
    "    E = torch.cat(embs).numpy() if embs else np.empty((0,512),np.float32)\n",
    "    if part is not None: E = np.vstack([part,E])\n",
    "    np.save(out_path, E); tmp.unlink(missing_ok=True)\n",
    "    dt=time.perf_counter()-t0\n",
    "    print(f\"[done] {out_path.name} {E.shape} ~{int(len(ids)/dt)} img/s\"); return E\n",
    "\n",
    "# ---- RUN (resumable; prints progress quickly) ----\n",
    "tr_emb = encode_split_solo(tr_ids, EMB/\"train_clip_openclip.npy\", batch_size=256, chunk=2_000)\n",
    "te_emb = encode_split_solo(te_ids, EMB/\"test_clip_openclip.npy\",  batch_size=256, chunk=2_000)\n",
    "print(\"Embeddings:\", tr_emb.shape, te_emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8babfe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_img_clip.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(r\"D:\\amazon ML challenge\\artifacts\")\n",
    "\n",
    "y = train[\"price\"].to_numpy(float)\n",
    "ridge_i = Ridge(alpha=1.0, random_state=42).fit(tr_emb, y)\n",
    "pred_img = np.clip(ridge_i.predict(te_emb), 1e-6, None)\n",
    "\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_img}).to_csv(ART/\"submission_img_clip.csv\", index=False)\n",
    "print(\"Saved:\", (ART/\"submission_img_clip.csv\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3091a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_ensemble_img_v1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(r\"D:\\amazon ML challenge\\artifacts\")\n",
    "df_txt = pd.read_csv(ART/\"submission_wc_log.csv\")      # your best log-target text model\n",
    "df_img = pd.read_csv(ART/\"submission_img_clip.csv\")    # just created\n",
    "\n",
    "m = df_txt.merge(df_img, on=\"sample_id\", suffixes=(\"_txt\",\"_img\"))\n",
    "m[\"price\"] = 0.9*m[\"price_txt\"] + 0.1*m[\"price_img\"]   # conservative blend\n",
    "m[[\"sample_id\",\"price\"]].to_csv(ART/\"submission_ensemble_img_v1.csv\", index=False)\n",
    "print(\"Saved:\", (ART/\"submission_ensemble_img_v1.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "262727b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE: D:\\amazon ML challenge\\artifacts\\test_out.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(r\"D:\\amazon ML challenge\")\n",
    "ART, DATA = ROOT/\"artifacts\", ROOT/\"data\"\n",
    "\n",
    "base = \"submission_ensemble_img_v1.csv\"  # swap to \"submission_wc_log.csv\" if you skip images\n",
    "df = pd.read_csv(ART/base)[[\"sample_id\",\"price\"]]\n",
    "df[\"price\"] = df[\"price\"].astype(float).clip(lower=1e-6)\n",
    "\n",
    "order = pd.read_csv(DATA/\"test.csv\")[\"sample_id\"]\n",
    "final = order.to_frame().merge(df, on=\"sample_id\", how=\"left\")\n",
    "assert final[\"price\"].notna().all()\n",
    "final.to_csv(ART/\"test_out.csv\", index=False)\n",
    "print(\"WROTE:\", (ART/\"test_out.csv\").resolve(), \"| rows:\", len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9030136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission_blend_text95_img5.csv\n",
      "Saved: submission_blend_text90_img9.csv\n",
      "Saved: submission_blend_text85_img15.csv\n",
      "Saved: submission_blend_text80_img19.csv\n"
     ]
    }
   ],
   "source": [
    "for w in (0.95, 0.9, 0.85, 0.8):\n",
    "    out = ART/f\"submission_blend_text{int(w*100)}_img{int((1-w)*100)}.csv\"\n",
    "    tmp = m.copy()\n",
    "    tmp[\"price\"] = w*tmp[\"price_txt\"] + (1-w)*tmp[\"price_img\"]\n",
    "    tmp[[\"sample_id\",\"price\"]].to_csv(out, index=False)\n",
    "    print(\"Saved:\", out.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
