{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea42ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal cleaner (only if catalog_content_clean doesn't exist yet)\n",
    "if \"catalog_content_clean\" not in locals() and \"catalog_content_clean\" not in train.columns if 'train' in locals() else [False]:\n",
    "    import re\n",
    "    def clean_text(s) -> str:\n",
    "        s = \"\" if s is None else str(s)\n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", s)\n",
    "        s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s)\n",
    "        return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282e8503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook CWD : d:\\amazon ML challenge\\notebooks\n",
      "Project ROOT : d:\\amazon ML challenge\n",
      "DATA exists? : True | True True\n",
      "ART path     : d:\\amazon ML challenge\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# Robust project-root + data loader for 02_stage2.ipynb\n",
    "\n",
    "import re, gc, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"Walk upward until we find a folder that actually contains data/train.csv (or common markers).\"\"\"\n",
    "    p = Path.cwd()\n",
    "    for _ in range(8):\n",
    "        # Preferred: explicit presence of data files\n",
    "        if (p / \"data\" / \"train.csv\").exists() and (p / \"data\" / \"test.csv\").exists():\n",
    "            return p\n",
    "        # Fallback: repo markers + data dir exists\n",
    "        if any((p / m).exists() for m in (\"requirements.txt\", \".git\", \"HANDOFF.md\")) and (p / \"data\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise FileNotFoundError(\"Could not locate project root containing a 'data/' folder.\")\n",
    "\n",
    "ROOT = find_project_root()\n",
    "DATA = ROOT / \"data\"\n",
    "ART  = ROOT / \"artifacts\"\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Notebook CWD :\", Path.cwd())\n",
    "print(\"Project ROOT :\", ROOT)\n",
    "print(\"DATA exists? :\", DATA.exists(), \"|\", (DATA / \"train.csv\").exists(), (DATA / \"test.csv\").exists())\n",
    "print(\"ART path     :\", ART)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(DATA / \"train.csv\")\n",
    "test  = pd.read_csv(DATA / \"test.csv\")\n",
    "\n",
    "# Choose text column (use cleaned if present, else raw)\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "\n",
    "# Safe string views for modeling\n",
    "X_text_tr = train[TEXT_COL].fillna(\"\").astype(str)\n",
    "X_text_te = test[TEXT_COL].fillna(\"\").astype(str)\n",
    "y = train[\"price\"].astype(float).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f813eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word+Char TF-IDF Ridge 5-fold MAE: 13.855923438301284 ± 0.11941516491554557\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "identity = FunctionTransformer(lambda s: s, validate=False)\n",
    "\n",
    "features = FeatureUnion([\n",
    "    (\"word\", Pipeline([\n",
    "        (\"id\", identity),\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"word\", ngram_range=(1,2),\n",
    "            max_features=350_000, min_df=2\n",
    "        ))\n",
    "    ])),\n",
    "    (\"char\", Pipeline([\n",
    "        (\"id\", identity),\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char_wb\", ngram_range=(3,6),\n",
    "            max_features=300_000, min_df=2\n",
    "        ))\n",
    "    ])),\n",
    "], n_jobs=1)\n",
    "\n",
    "ridge = Ridge(alpha=1.2, random_state=42)\n",
    "\n",
    "pipe_wc = Pipeline([\n",
    "    (\"features\", features),\n",
    "    (\"ridge\", ridge),\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "maes = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    pred = pipe_wc.predict(X_text_tr.iloc[va])\n",
    "    maes.append(mean_absolute_error(y[va], pred))\n",
    "print(\"Word+Char TF-IDF Ridge 5-fold MAE:\", np.mean(maes), \"±\", np.std(maes))\n",
    "\n",
    "# Fit full and write submission\n",
    "pipe_wc.fit(X_text_tr, y)\n",
    "pred_wc = pipe_wc.predict(X_text_te)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_wc}).to_csv(ART / \"submission_wc.csv\", index=False)\n",
    "print(\"Saved:\", (ART / \"submission_wc.csv\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e3b4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_word_meta.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "# === Stage-2B minimal: word TF-IDF + cheap meta features → pred_tm + save CSV ===\n",
    "from pathlib import Path\n",
    "import re, numpy as np, pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Resolve paths and data (works whether you're in notebooks/ or project root)\n",
    "ROOT = globals().get(\"ROOT\", Path.cwd())\n",
    "ROOT = ROOT if isinstance(ROOT, Path) else Path(ROOT)\n",
    "DATA = Path(globals().get(\"DATA\", ROOT / \"data\"))\n",
    "ART  = Path(globals().get(\"ART\",  ROOT / \"artifacts\"))\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data if not already present\n",
    "if \"train\" not in globals() or \"test\" not in globals():\n",
    "    train = pd.read_csv(DATA / \"train.csv\")\n",
    "    test  = pd.read_csv(DATA / \"test.csv\")\n",
    "\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "X_text_tr = train[TEXT_COL].fillna(\"\").astype(str)\n",
    "X_text_te = test[TEXT_COL].fillna(\"\").astype(str)\n",
    "y = train[\"price\"].astype(float).values\n",
    "\n",
    "# ---- meta feature extractor ----\n",
    "def extract_meta(s: pd.Series) -> pd.DataFrame:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "\n",
    "    nums = s.str.findall(r\"(?<![a-zA-Z])(\\d+(?:\\.\\d+)?)\")\n",
    "    num_count = nums.apply(len).astype(float)\n",
    "    max_num = nums.apply(lambda xs: max(map(float, xs)) if xs else np.nan).astype(float)\n",
    "    min_num = nums.apply(lambda xs: min(map(float, xs)) if xs else np.nan).astype(float)\n",
    "\n",
    "    pack = s.str.extract(r\"(?:pack of|pack)\\s*(\\d+)|(\\d+)\\s*-\\s*pack|(\\d+)\\s*pack\", expand=True)\n",
    "    pack_n = pack.apply(lambda row: next((int(x) for x in row if pd.notna(x)), np.nan), axis=1).astype(float)\n",
    "\n",
    "    uw = s.str.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(ml|l|oz|g|kg|lb)\")\n",
    "    def norm_units(pairs):\n",
    "        ml = g = None\n",
    "        for val, unit in pairs:\n",
    "            v = float(val)\n",
    "            if unit == \"ml\": ml = (ml or 0) + v\n",
    "            elif unit == \"l\":  ml = (ml or 0) + v*1000\n",
    "            elif unit == \"oz\": g  = (g  or 0) + v*28.3495\n",
    "            elif unit == \"g\":  g  = (g  or 0) + v\n",
    "            elif unit == \"kg\": g  = (g  or 0) + v*1000\n",
    "            elif unit == \"lb\": g  = (g  or 0) + v*453.592\n",
    "        return pd.Series({\"vol_ml\": ml if ml is not None else np.nan,\n",
    "                          \"wt_g\":  g  if g  is not None else np.nan})\n",
    "    unit_df = uw.apply(norm_units)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"num_count\": num_count,\n",
    "        \"max_num\": max_num,\n",
    "        \"min_num\": min_num,\n",
    "        \"pack_n\":  pack_n,\n",
    "    })\n",
    "    df = pd.concat([df, unit_df], axis=1).fillna(0.0)\n",
    "    return df\n",
    "\n",
    "meta_tr = extract_meta(X_text_tr)\n",
    "meta_te = extract_meta(X_text_te)\n",
    "\n",
    "train_aug = pd.DataFrame({TEXT_COL: X_text_tr})\n",
    "test_aug  = pd.DataFrame({TEXT_COL: X_text_te})\n",
    "for c in meta_tr.columns:\n",
    "    train_aug[c] = meta_tr[c]\n",
    "    test_aug[c]  = meta_te[c]\n",
    "\n",
    "numeric_cols = meta_tr.columns.tolist()\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"tfidf_word\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=300_000), TEXT_COL),\n",
    "    (\"num\", Pipeline([(\"scale\", StandardScaler(with_mean=False))]), numeric_cols),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "model = Ridge(alpha=1.0, random_state=42)\n",
    "pipe_tm = Pipeline([(\"ct\", ct), (\"ridge\", model)])\n",
    "\n",
    "# Fit full + predict\n",
    "pipe_tm.fit(train_aug, y)\n",
    "pred_tm = pipe_tm.predict(test_aug)\n",
    "\n",
    "# Save submission for blending\n",
    "out_tm = ART / \"submission_word_meta.csv\"\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_tm}).to_csv(out_tm, index=False)\n",
    "print(\"Saved:\", out_tm.resolve(), \"| rows:\", len(pred_tm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584923f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_ensemble_v1.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "# Simple average of the two Stage-2 submissions\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(ART)  # reuse from earlier if defined\n",
    "df_wc = pd.read_csv(ART / \"submission_wc.csv\")\n",
    "df_tm = pd.read_csv(ART / \"submission_word_meta.csv\")\n",
    "\n",
    "df = df_wc.merge(df_tm, on=\"sample_id\", suffixes=(\"_wc\", \"_tm\"))\n",
    "df[\"price\"] = 0.5*df[\"price_wc\"] + 0.5*df[\"price_tm\"]\n",
    "out = ART / \"submission_ensemble_v1.csv\"\n",
    "df[[\"sample_id\",\"price\"]].to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve(), \"| rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fc62221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ART = D:\\amazon ML challenge\\artifacts\n",
      "Available submissions: ['submission_ensemble_v1.csv', 'submission_final.csv', 'submission_wc.csv', 'submission_word_meta.csv']\n",
      "-57.24441578355 427.01332224186393 19.36302922582783\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse ART if defined; else resolve it from project root\n",
    "ART = Path(globals().get(\"ART\", Path.cwd() / \"artifacts\"))\n",
    "\n",
    "print(\"ART =\", ART.resolve())\n",
    "print(\"Available submissions:\", [p.name for p in ART.glob(\"submission_*.csv\")])\n",
    "\n",
    "df = pd.read_csv(ART / \"submission_ensemble_v1.csv\")\n",
    "assert len(df)==75000 and df[\"sample_id\"].is_unique and not df[\"price\"].isna().any()\n",
    "print(df[\"price\"].min(), df[\"price\"].max(), df[\"price\"].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aa09661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 147/147 [01:09<00:00,  2.12it/s]\n",
      "Batches: 100%|██████████| 147/147 [01:09<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_ensemble_v2.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ART = Path(globals().get(\"ART\", Path.cwd() / \"artifacts\"))\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "\n",
    "# Encode\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tr_vec = model.encode(train[TEXT_COL].fillna(\"\").tolist(), batch_size=512, show_progress_bar=True, convert_to_numpy=True)\n",
    "te_vec = model.encode(test[TEXT_COL].fillna(\"\").tolist(),  batch_size=512, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Fit + predict\n",
    "ridge_emb = Ridge(alpha=1.0, random_state=42).fit(tr_vec, train[\"price\"].values)\n",
    "pred_emb = ridge_emb.predict(te_vec)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_emb}).to_csv(ART/\"submission_sbert.csv\", index=False)\n",
    "\n",
    "# 3-way blend with existing files\n",
    "df_wc = pd.read_csv(ART/\"submission_wc.csv\")\n",
    "df_tm = pd.read_csv(ART/\"submission_word_meta.csv\")\n",
    "df_sb = pd.read_csv(ART/\"submission_sbert.csv\")\n",
    "\n",
    "m = df_wc.merge(df_tm, on=\"sample_id\", suffixes=(\"_wc\",\"_tm\")).merge(df_sb, on=\"sample_id\")\n",
    "m[\"price\"] = 0.4*m[\"price_wc\"] + 0.4*m[\"price_tm\"] + 0.2*m[\"price\"]\n",
    "out = ART/\"submission_ensemble_v2.csv\"\n",
    "m[[\"sample_id\",\"price\"]].to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve(), \"| rows:\", len(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7af016d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m maes = []\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tr, va \u001b[38;5;129;01min\u001b[39;00m cv.split(X_text_tr):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[43mpipe_wc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_text_tr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     pred = pipe_wc.predict(X_text_tr.iloc[va])\n\u001b[32m     44\u001b[39m     maes.append(mean_absolute_error(y[va], pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:1248\u001b[39m, in \u001b[36mRidge.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1237\u001b[39m xp, _ = get_namespace(X, y, sample_weight)\n\u001b[32m   1238\u001b[39m X, y = validate_data(\n\u001b[32m   1239\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1240\u001b[39m     X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1246\u001b[39m     y_numeric=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1247\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:990\u001b[39m, in \u001b[36m_BaseRidge.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    986\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    987\u001b[39m         \u001b[38;5;66;03m# for dense matrices or when intercept is set to 0\u001b[39;00m\n\u001b[32m    988\u001b[39m         params = {}\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     \u001b[38;5;28mself\u001b[39m.coef_, \u001b[38;5;28mself\u001b[39m.n_iter_, \u001b[38;5;28mself\u001b[39m.solver_ = \u001b[43m_ridge_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_solver\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_intercept(X_offset, y_offset, X_scale)\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:712\u001b[39m, in \u001b[36m_ridge_regression\u001b[39m\u001b[34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, positive, random_state, return_n_iter, return_intercept, return_solver, X_scale, X_offset, check_input, fit_intercept)\u001b[39m\n\u001b[32m    710\u001b[39m n_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m solver == \u001b[33m\"\u001b[39m\u001b[33msparse_cg\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m     coef = \u001b[43m_solve_sparse_cg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight_sqrt\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight_sqrt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhas_sw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m solver == \u001b[33m\"\u001b[39m\u001b[33mlsqr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    725\u001b[39m     coef, n_iter = _solve_lsqr(\n\u001b[32m    726\u001b[39m         X,\n\u001b[32m    727\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    734\u001b[39m         sample_weight_sqrt=sample_weight_sqrt \u001b[38;5;28;01mif\u001b[39;00m has_sw \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    735\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:122\u001b[39m, in \u001b[36m_solve_sparse_cg\u001b[39m\u001b[34m(X, y, alpha, max_iter, tol, verbose, X_offset, X_scale, sample_weight_sqrt)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features > n_samples:\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# kernel ridge\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# w = X.T * inv(X X^t + alpha*Id) y\u001b[39;00m\n\u001b[32m    119\u001b[39m     C = sp_linalg.LinearOperator(\n\u001b[32m    120\u001b[39m         (n_samples, n_samples), matvec=mv, dtype=X.dtype\n\u001b[32m    121\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     coef, info = \u001b[43m_sparse_linalg_cg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     coefs[i] = X1.rmatvec(coef)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m# linear ridge\u001b[39;00m\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# w = inv(X^t X + alpha*Id) * X.T y\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\iterative.py:415\u001b[39m, in \u001b[36mcg\u001b[39m\u001b[34m(A, b, x0, rtol, atol, maxiter, M, callback)\u001b[39m\n\u001b[32m    412\u001b[39m     p = np.empty_like(r)\n\u001b[32m    413\u001b[39m     p[:] = z[:]\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m q = \u001b[43mmatvec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m alpha = rho_cur / dotprod(p, q)\n\u001b[32m    417\u001b[39m x += alpha*p\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:258\u001b[39m, in \u001b[36mLinearOperator.matvec\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape != (N,) \u001b[38;5;129;01mand\u001b[39;00m x.shape != (N,\u001b[32m1\u001b[39m):\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mdimension mismatch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matvec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.matrix):\n\u001b[32m    261\u001b[39m     y = asmatrix(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:616\u001b[39m, in \u001b[36m_CustomLinearOperator._matvec\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_matvec\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__matvec_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:100\u001b[39m, in \u001b[36m_solve_sparse_cg.<locals>.create_mv.<locals>._mv\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_mv\u001b[39m(x):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X1.matvec(\u001b[43mX1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmatvec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) + curr_alpha * x\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:305\u001b[39m, in \u001b[36mLinearOperator.rmatvec\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape != (M,) \u001b[38;5;129;01mand\u001b[39;00m x.shape != (M,\u001b[32m1\u001b[39m):\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mdimension mismatch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rmatvec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.matrix):\n\u001b[32m    308\u001b[39m     y = asmatrix(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:622\u001b[39m, in \u001b[36m_CustomLinearOperator._rmatvec\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mrmatvec is not defined\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__rmatvec_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:66\u001b[39m, in \u001b[36m_get_rescaled_operator.<locals>.rmatvec\u001b[39m\u001b[34m(b)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrmatvec\u001b[39m(b):\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m - X_offset * b.dot(sample_weight_sqrt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:600\u001b[39m, in \u001b[36m_spbase.dot\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m * other\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:908\u001b[39m, in \u001b[36m_spbase.__matmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScalar operands are not allowed, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    907\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33muse \u001b[39m\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matmul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:793\u001b[39m, in \u001b[36m_spbase._matmul_dispatch\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m other.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m np.ndarray:\n\u001b[32m    791\u001b[39m     \u001b[38;5;66;03m# Fast path for the most common case\u001b[39;00m\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m other.shape == (N,):\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matmul_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m other.shape == (N, \u001b[32m1\u001b[39m):\n\u001b[32m    795\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._matmul_vector(other.ravel())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\amazon ML challenge\\.venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:395\u001b[39m, in \u001b[36m_cs_matrix._matmul_vector\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# csr_matvec or csc_matvec\u001b[39;00m\n\u001b[32m    394\u001b[39m fn = \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m.format + \u001b[33m'\u001b[39m\u001b[33m_matvec\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Identity transformer so FeatureUnion can feed the same text to two vectorizers\n",
    "identity = FunctionTransformer(lambda s: s, validate=False)\n",
    "\n",
    "features = FeatureUnion([\n",
    "    (\"word\", Pipeline([\n",
    "        (\"id\", identity),\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            ngram_range=(1,2),\n",
    "            max_features=350_000,\n",
    "            min_df=2\n",
    "        ))\n",
    "    ])),\n",
    "    (\"char\", Pipeline([\n",
    "        (\"id\", identity),\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char_wb\",\n",
    "            ngram_range=(3,6),\n",
    "            max_features=300_000,\n",
    "            min_df=2\n",
    "        ))\n",
    "    ])),\n",
    "], n_jobs=1)  # keep 1 for RAM safety\n",
    "\n",
    "ridge = Ridge(alpha=1.2, random_state=42)\n",
    "\n",
    "pipe_wc = Pipeline([\n",
    "    (\"features\", features),\n",
    "    (\"ridge\", ridge),\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "maes = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    pred = pipe_wc.predict(X_text_tr.iloc[va])\n",
    "    maes.append(mean_absolute_error(y[va], pred))\n",
    "print(\"Word+Char TF-IDF Ridge 5-fold MAE:\", np.mean(maes), \"±\", np.std(maes))\n",
    "\n",
    "# Fit full and predict\n",
    "pipe_wc.fit(X_text_tr, y)\n",
    "pred_wc = pipe_wc.predict(X_text_te)\n",
    "\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_wc}) \\\n",
    "  .to_csv(ART / \"submission_wc.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543cc667",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# fill NaNs with 0 for modeling\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.fillna(\u001b[32m0.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m meta_tr = extract_meta(\u001b[43mtrain\u001b[49m[TEXT_COL])\n\u001b[32m     41\u001b[39m meta_te = extract_meta(test[TEXT_COL])\n\u001b[32m     43\u001b[39m meta_tr.head(), meta_te.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_meta(s: pd.Series) -> pd.DataFrame:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "    # numbers present\n",
    "    nums = s.str.findall(r\"(?<![a-zA-Z])(\\d+(?:\\.\\d+)?)\")\n",
    "    num_count = nums.apply(len)\n",
    "    max_num = nums.apply(lambda xs: max(map(float, xs)) if xs else np.nan)\n",
    "    min_num = nums.apply(lambda xs: min(map(float, xs)) if xs else np.nan)\n",
    "\n",
    "    # pack size: \"pack of 2\", \"2-pack\", \"2 pack\"\n",
    "    pack = s.str.extract(r\"(?:pack of|pack)\\s*(\\d+)|(\\d+)\\s*-\\s*pack|(\\d+)\\s*pack\", expand=True)\n",
    "    pack_n = pack.apply(lambda row: next((int(x) for x in row if pd.notna(x)), np.nan), axis=1)\n",
    "\n",
    "    # volumes/weights normalized\n",
    "    # capture '500 ml', '0.5 l', '12 oz', '250 g', '1 kg', '1 lb'\n",
    "    uw = s.str.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(ml|l|oz|g|kg|lb)\")\n",
    "    def norm_units(pairs):\n",
    "        ml = g = None\n",
    "        for val, unit in pairs:\n",
    "            v = float(val)\n",
    "            if unit == \"ml\": ml = (ml or 0) + v\n",
    "            elif unit == \"l\":  ml = (ml or 0) + v*1000\n",
    "            elif unit == \"oz\": g  = (g  or 0) + v*28.3495\n",
    "            elif unit == \"g\":  g  = (g  or 0) + v\n",
    "            elif unit == \"kg\": g  = (g  or 0) + v*1000\n",
    "            elif unit == \"lb\": g  = (g  or 0) + v*453.592\n",
    "        return pd.Series({\"vol_ml\": ml if ml is not None else np.nan,\n",
    "                          \"wt_g\":  g  if g  is not None else np.nan})\n",
    "    unit_df = uw.apply(norm_units)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"num_count\": num_count.astype(float),\n",
    "        \"max_num\": max_num.astype(float),\n",
    "        \"min_num\": min_num.astype(float),\n",
    "        \"pack_n\":  pack_n.astype(float),\n",
    "    })\n",
    "    df = pd.concat([df, unit_df], axis=1)\n",
    "    # fill NaNs with 0 for modeling\n",
    "    return df.fillna(0.0)\n",
    "\n",
    "meta_tr = extract_meta(train[TEXT_COL])\n",
    "meta_te = extract_meta(test[TEXT_COL])\n",
    "\n",
    "meta_tr.head(), meta_te.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7582975",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT_COL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Build a DataFrame combining text + meta\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_aug = pd.DataFrame({\u001b[43mTEXT_COL\u001b[49m: X_text_tr})\n\u001b[32m      7\u001b[39m test_aug  = pd.DataFrame({TEXT_COL: X_text_te})\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m meta_tr.columns:\n",
      "\u001b[31mNameError\u001b[39m: name 'TEXT_COL' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build a DataFrame combining text + meta\n",
    "train_aug = pd.DataFrame({TEXT_COL: X_text_tr})\n",
    "test_aug  = pd.DataFrame({TEXT_COL: X_text_te})\n",
    "for c in meta_tr.columns:\n",
    "    train_aug[c] = meta_tr[c]\n",
    "    test_aug[c]  = meta_te[c]\n",
    "\n",
    "numeric_cols = meta_tr.columns.tolist()\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"tfidf_word\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=300_000), TEXT_COL),\n",
    "    (\"num\", Pipeline([(\"scale\", StandardScaler(with_mean=False))]), numeric_cols),\n",
    "], remainder=\"drop\", n_jobs=None)\n",
    "\n",
    "model = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "pipe_tm = Pipeline([\n",
    "    (\"ct\", ct),\n",
    "    (\"ridge\", model),\n",
    "])\n",
    "\n",
    "maes=[]\n",
    "for tr, va in cv.split(train_aug):\n",
    "    pipe_tm.fit(train_aug.iloc[tr], y[tr])\n",
    "    pp = pipe_tm.predict(train_aug.iloc[va])\n",
    "    maes.append(mean_absolute_error(y[va], pp))\n",
    "print(\"TF-IDF(word)+Meta Ridge 5-fold MAE:\", np.mean(maes), \"±\", np.std(maes))\n",
    "\n",
    "pipe_tm.fit(train_aug, y)\n",
    "pred_tm = pipe_tm.predict(test_aug)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_tm}) \\\n",
    "  .to_csv(ART / \"submission_word_meta.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9010c19e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# We already have pred_wc (word+char) and pred_tm (word+meta).\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Uniform average is a solid start:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ens_pred = \u001b[32m0.5\u001b[39m * \u001b[43mpred_wc\u001b[49m + \u001b[32m0.5\u001b[39m * pred_tm\n\u001b[32m      4\u001b[39m pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33msample_id\u001b[39m\u001b[33m\"\u001b[39m: test[\u001b[33m\"\u001b[39m\u001b[33msample_id\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m\"\u001b[39m: ens_pred}) \\\n\u001b[32m      5\u001b[39m   .to_csv(ART / \u001b[33m\"\u001b[39m\u001b[33msubmission_ensemble_v1.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pred_wc' is not defined"
     ]
    }
   ],
   "source": [
    "# We already have pred_wc (word+char) and pred_tm (word+meta).\n",
    "# Uniform average is a solid start:\n",
    "ens_pred = 0.5 * pred_wc + 0.5 * pred_tm\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": ens_pred}) \\\n",
    "  .to_csv(ART / \"submission_ensemble_v1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500bf7a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# pip install -U sentence-transformers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ridge\n\u001b[32m      5\u001b[39m emb_model = SentenceTransformer(\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# 384-dim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# pip install -U sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "emb_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # 384-dim\n",
    "\n",
    "# Encode in manageable batches\n",
    "tr_emb = emb_model.encode(X_text_tr.tolist(), batch_size=512, show_progress_bar=True, convert_to_numpy=True)\n",
    "te_emb = emb_model.encode(X_text_te.tolist(), batch_size=512, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "ridge_emb = Ridge(alpha=1.0, random_state=42)\n",
    "maes=[]\n",
    "for tr, va in cv.split(tr_emb):\n",
    "    ridge_emb.fit(tr_emb[tr], y[tr])\n",
    "    pp = ridge_emb.predict(tr_emb[va])\n",
    "    maes.append(mean_absolute_error(y[va], pp))\n",
    "print(\"Sentence-Embeddings Ridge 5-fold MAE:\", np.mean(maes), \"±\", np.std(maes))\n",
    "\n",
    "ridge_emb.fit(tr_emb, y)\n",
    "pred_emb = ridge_emb.predict(te_emb)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_emb}) \\\n",
    "  .to_csv(ART / \"submission_sbert.csv\", index=False)\n",
    "\n",
    "# Try a 3-way blend\n",
    "pred_blend3 = 0.4*pred_wc + 0.4*pred_tm + 0.2*pred_emb\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_blend3}) \\\n",
    "  .to_csv(ART / \"submission_ensemble_v2.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
