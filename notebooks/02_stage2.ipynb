{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea42ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal cleaner (only if catalog_content_clean doesn't exist yet)\n",
    "if \"catalog_content_clean\" not in locals() and \"catalog_content_clean\" not in train.columns if 'train' in locals() else [False]:\n",
    "    import re\n",
    "    def clean_text(s) -> str:\n",
    "        s = \"\" if s is None else str(s)\n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", s)\n",
    "        s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s)\n",
    "        return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282e8503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook CWD : d:\\amazon ML challenge\\notebooks\n",
      "Project ROOT : d:\\amazon ML challenge\n",
      "DATA exists? : True | True True\n",
      "ART path     : d:\\amazon ML challenge\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# Robust project-root + data loader for 02_stage2.ipynb\n",
    "\n",
    "import re, gc, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"Walk upward until we find a folder that actually contains data/train.csv (or common markers).\"\"\"\n",
    "    p = Path.cwd()\n",
    "    for _ in range(8):\n",
    "        # Preferred: explicit presence of data files\n",
    "        if (p / \"data\" / \"train.csv\").exists() and (p / \"data\" / \"test.csv\").exists():\n",
    "            return p\n",
    "        # Fallback: repo markers + data dir exists\n",
    "        if any((p / m).exists() for m in (\"requirements.txt\", \".git\", \"HANDOFF.md\")) and (p / \"data\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise FileNotFoundError(\"Could not locate project root containing a 'data/' folder.\")\n",
    "\n",
    "ROOT = find_project_root()\n",
    "DATA = ROOT / \"data\"\n",
    "ART  = ROOT / \"artifacts\"\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Notebook CWD :\", Path.cwd())\n",
    "print(\"Project ROOT :\", ROOT)\n",
    "print(\"DATA exists? :\", DATA.exists(), \"|\", (DATA / \"train.csv\").exists(), (DATA / \"test.csv\").exists())\n",
    "print(\"ART path     :\", ART)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(DATA / \"train.csv\")\n",
    "test  = pd.read_csv(DATA / \"test.csv\")\n",
    "\n",
    "# Choose text column (use cleaned if present, else raw)\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "\n",
    "# Safe string views for modeling\n",
    "X_text_tr = train[TEXT_COL].fillna(\"\").astype(str)\n",
    "X_text_te = test[TEXT_COL].fillna(\"\").astype(str)\n",
    "y = train[\"price\"].astype(float).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f813eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word+Char TF-IDF Ridge 5-fold MAE: 13.855923438301284 ± 0.11941516491554557\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "identity = FunctionTransformer(lambda s: s, validate=False)\n",
    "\n",
    "features = FeatureUnion([\n",
    "    (\"word\", Pipeline([\n",
    "        (\"id\", identity),\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"word\", ngram_range=(1,2),\n",
    "            max_features=350_000, min_df=2\n",
    "        ))\n",
    "    ])),\n",
    "    (\"char\", Pipeline([\n",
    "        (\"id\", identity),\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char_wb\", ngram_range=(3,6),\n",
    "            max_features=300_000, min_df=2\n",
    "        ))\n",
    "    ])),\n",
    "], n_jobs=1)\n",
    "\n",
    "ridge = Ridge(alpha=1.2, random_state=42)\n",
    "\n",
    "pipe_wc = Pipeline([\n",
    "    (\"features\", features),\n",
    "    (\"ridge\", ridge),\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "maes = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    pred = pipe_wc.predict(X_text_tr.iloc[va])\n",
    "    maes.append(mean_absolute_error(y[va], pred))\n",
    "print(\"Word+Char TF-IDF Ridge 5-fold MAE:\", np.mean(maes), \"±\", np.std(maes))\n",
    "\n",
    "# Fit full and write submission\n",
    "pipe_wc.fit(X_text_tr, y)\n",
    "pred_wc = pipe_wc.predict(X_text_te)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_wc}).to_csv(ART / \"submission_wc.csv\", index=False)\n",
    "print(\"Saved:\", (ART / \"submission_wc.csv\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e3b4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_word_meta.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "# === Stage-2B minimal: word TF-IDF + cheap meta features → pred_tm + save CSV ===\n",
    "from pathlib import Path\n",
    "import re, numpy as np, pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Resolve paths and data (works whether you're in notebooks/ or project root)\n",
    "ROOT = globals().get(\"ROOT\", Path.cwd())\n",
    "ROOT = ROOT if isinstance(ROOT, Path) else Path(ROOT)\n",
    "DATA = Path(globals().get(\"DATA\", ROOT / \"data\"))\n",
    "ART  = Path(globals().get(\"ART\",  ROOT / \"artifacts\"))\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data if not already present\n",
    "if \"train\" not in globals() or \"test\" not in globals():\n",
    "    train = pd.read_csv(DATA / \"train.csv\")\n",
    "    test  = pd.read_csv(DATA / \"test.csv\")\n",
    "\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "X_text_tr = train[TEXT_COL].fillna(\"\").astype(str)\n",
    "X_text_te = test[TEXT_COL].fillna(\"\").astype(str)\n",
    "y = train[\"price\"].astype(float).values\n",
    "\n",
    "# ---- meta feature extractor ----\n",
    "def extract_meta(s: pd.Series) -> pd.DataFrame:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "\n",
    "    nums = s.str.findall(r\"(?<![a-zA-Z])(\\d+(?:\\.\\d+)?)\")\n",
    "    num_count = nums.apply(len).astype(float)\n",
    "    max_num = nums.apply(lambda xs: max(map(float, xs)) if xs else np.nan).astype(float)\n",
    "    min_num = nums.apply(lambda xs: min(map(float, xs)) if xs else np.nan).astype(float)\n",
    "\n",
    "    pack = s.str.extract(r\"(?:pack of|pack)\\s*(\\d+)|(\\d+)\\s*-\\s*pack|(\\d+)\\s*pack\", expand=True)\n",
    "    pack_n = pack.apply(lambda row: next((int(x) for x in row if pd.notna(x)), np.nan), axis=1).astype(float)\n",
    "\n",
    "    uw = s.str.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(ml|l|oz|g|kg|lb)\")\n",
    "    def norm_units(pairs):\n",
    "        ml = g = None\n",
    "        for val, unit in pairs:\n",
    "            v = float(val)\n",
    "            if unit == \"ml\": ml = (ml or 0) + v\n",
    "            elif unit == \"l\":  ml = (ml or 0) + v*1000\n",
    "            elif unit == \"oz\": g  = (g  or 0) + v*28.3495\n",
    "            elif unit == \"g\":  g  = (g  or 0) + v\n",
    "            elif unit == \"kg\": g  = (g  or 0) + v*1000\n",
    "            elif unit == \"lb\": g  = (g  or 0) + v*453.592\n",
    "        return pd.Series({\"vol_ml\": ml if ml is not None else np.nan,\n",
    "                          \"wt_g\":  g  if g  is not None else np.nan})\n",
    "    unit_df = uw.apply(norm_units)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"num_count\": num_count,\n",
    "        \"max_num\": max_num,\n",
    "        \"min_num\": min_num,\n",
    "        \"pack_n\":  pack_n,\n",
    "    })\n",
    "    df = pd.concat([df, unit_df], axis=1).fillna(0.0)\n",
    "    return df\n",
    "\n",
    "meta_tr = extract_meta(X_text_tr)\n",
    "meta_te = extract_meta(X_text_te)\n",
    "\n",
    "train_aug = pd.DataFrame({TEXT_COL: X_text_tr})\n",
    "test_aug  = pd.DataFrame({TEXT_COL: X_text_te})\n",
    "for c in meta_tr.columns:\n",
    "    train_aug[c] = meta_tr[c]\n",
    "    test_aug[c]  = meta_te[c]\n",
    "\n",
    "numeric_cols = meta_tr.columns.tolist()\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"tfidf_word\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=300_000), TEXT_COL),\n",
    "    (\"num\", Pipeline([(\"scale\", StandardScaler(with_mean=False))]), numeric_cols),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "model = Ridge(alpha=1.0, random_state=42)\n",
    "pipe_tm = Pipeline([(\"ct\", ct), (\"ridge\", model)])\n",
    "\n",
    "# Fit full + predict\n",
    "pipe_tm.fit(train_aug, y)\n",
    "pred_tm = pipe_tm.predict(test_aug)\n",
    "\n",
    "# Save submission for blending\n",
    "out_tm = ART / \"submission_word_meta.csv\"\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_tm}).to_csv(out_tm, index=False)\n",
    "print(\"Saved:\", out_tm.resolve(), \"| rows:\", len(pred_tm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584923f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_ensemble_v1.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "# Simple average of the two Stage-2 submissions\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(ART)  # reuse from earlier if defined\n",
    "df_wc = pd.read_csv(ART / \"submission_wc.csv\")\n",
    "df_tm = pd.read_csv(ART / \"submission_word_meta.csv\")\n",
    "\n",
    "df = df_wc.merge(df_tm, on=\"sample_id\", suffixes=(\"_wc\", \"_tm\"))\n",
    "df[\"price\"] = 0.5*df[\"price_wc\"] + 0.5*df[\"price_tm\"]\n",
    "out = ART / \"submission_ensemble_v1.csv\"\n",
    "df[[\"sample_id\",\"price\"]].to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve(), \"| rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fc62221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ART = D:\\amazon ML challenge\\artifacts\n",
      "Available submissions: ['submission_ensemble_v1.csv', 'submission_final.csv', 'submission_wc.csv', 'submission_word_meta.csv']\n",
      "-57.24441578355 427.01332224186393 19.36302922582783\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse ART if defined; else resolve it from project root\n",
    "ART = Path(globals().get(\"ART\", Path.cwd() / \"artifacts\"))\n",
    "\n",
    "print(\"ART =\", ART.resolve())\n",
    "print(\"Available submissions:\", [p.name for p in ART.glob(\"submission_*.csv\")])\n",
    "\n",
    "df = pd.read_csv(ART / \"submission_ensemble_v1.csv\")\n",
    "assert len(df)==75000 and df[\"sample_id\"].is_unique and not df[\"price\"].isna().any()\n",
    "print(df[\"price\"].min(), df[\"price\"].max(), df[\"price\"].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aa09661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 147/147 [01:09<00:00,  2.12it/s]\n",
      "Batches: 100%|██████████| 147/147 [01:09<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_ensemble_v2.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ART = Path(globals().get(\"ART\", Path.cwd() / \"artifacts\"))\n",
    "TEXT_COL = \"catalog_content_clean\" if \"catalog_content_clean\" in train.columns else \"catalog_content\"\n",
    "\n",
    "# Encode\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tr_vec = model.encode(train[TEXT_COL].fillna(\"\").tolist(), batch_size=512, show_progress_bar=True, convert_to_numpy=True)\n",
    "te_vec = model.encode(test[TEXT_COL].fillna(\"\").tolist(),  batch_size=512, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Fit + predict\n",
    "ridge_emb = Ridge(alpha=1.0, random_state=42).fit(tr_vec, train[\"price\"].values)\n",
    "pred_emb = ridge_emb.predict(te_vec)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_emb}).to_csv(ART/\"submission_sbert.csv\", index=False)\n",
    "\n",
    "# 3-way blend with existing files\n",
    "df_wc = pd.read_csv(ART/\"submission_wc.csv\")\n",
    "df_tm = pd.read_csv(ART/\"submission_word_meta.csv\")\n",
    "df_sb = pd.read_csv(ART/\"submission_sbert.csv\")\n",
    "\n",
    "m = df_wc.merge(df_tm, on=\"sample_id\", suffixes=(\"_wc\",\"_tm\")).merge(df_sb, on=\"sample_id\")\n",
    "m[\"price\"] = 0.4*m[\"price_wc\"] + 0.4*m[\"price_tm\"] + 0.2*m[\"price\"]\n",
    "out = ART/\"submission_ensemble_v2.csv\"\n",
    "m[[\"sample_id\",\"price\"]].to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve(), \"| rows:\", len(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "361117ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-6):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / denom) * 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8254dc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SMAPE (word+char, log target): 52.18% ± 0.38%\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_log.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse your word+char FeatureUnion pipe from Stage-2A\n",
    "wc_features = pipe_wc.named_steps[\"features\"]  # same vectorizers\n",
    "base = Ridge(alpha=1.2, random_state=42)\n",
    "\n",
    "log_ridge = TransformedTargetRegressor(\n",
    "    regressor=Ridge(alpha=1.2, random_state=42),\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "\n",
    "pipe_wc_log = Pipeline([\n",
    "    (\"features\", wc_features),\n",
    "    (\"ridge\", log_ridge),\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc_log.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    p = pipe_wc_log.predict(X_text_tr.iloc[va]).clip(min=1e-6)\n",
    "    scores.append(smape(y[va], p))\n",
    "print(f\"CV SMAPE (word+char, log target): {np.mean(scores):.2f}% ± {np.std(scores):.2f}%\")\n",
    "\n",
    "# Fit full + save submission\n",
    "pipe_wc_log.fit(X_text_tr, y)\n",
    "pred_wc_log = pipe_wc_log.predict(X_text_te).clip(min=1e-6)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_wc_log}).to_csv(ART/\"submission_wc_log.csv\", index=False)\n",
    "print(\"Saved:\", (ART/\"submission_wc_log.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e6f928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SMAPE (word+char, log target): 52.18% ± 0.38%\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_log.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse your word+char FeatureUnion pipe from Stage-2A\n",
    "wc_features = pipe_wc.named_steps[\"features\"]  # same vectorizers\n",
    "base = Ridge(alpha=1.2, random_state=42)\n",
    "\n",
    "log_ridge = TransformedTargetRegressor(\n",
    "    regressor=Ridge(alpha=1.2, random_state=42),\n",
    "    func=np.log1p,\n",
    "    inverse_func=np.expm1\n",
    ")\n",
    "\n",
    "pipe_wc_log = Pipeline([\n",
    "    (\"features\", wc_features),\n",
    "    (\"ridge\", log_ridge),\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc_log.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    p = pipe_wc_log.predict(X_text_tr.iloc[va]).clip(min=1e-6)\n",
    "    scores.append(smape(y[va], p))\n",
    "print(f\"CV SMAPE (word+char, log target): {np.mean(scores):.2f}% ± {np.std(scores):.2f}%\")\n",
    "\n",
    "# Fit full + save submission\n",
    "pipe_wc_log.fit(X_text_tr, y)\n",
    "pred_wc_log = pipe_wc_log.predict(X_text_te).clip(min=1e-6)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_wc_log}).to_csv(ART/\"submission_wc_log.csv\", index=False)\n",
    "print(\"Saved:\", (ART/\"submission_wc_log.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f483312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SMAPE (word+char, Gamma GLM): 64.09% ± 0.47%\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_gamma.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "pipe_wc_gamma = Pipeline([\n",
    "    (\"features\", wc_features),\n",
    "    (\"glm\", TweedieRegressor(power=2.0, link=\"log\", alpha=1e-3, max_iter=3000, tol=1e-6, warm_start=True)),\n",
    "])\n",
    "\n",
    "scores = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc_gamma.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    p = pipe_wc_gamma.predict(X_text_tr.iloc[va]).clip(min=1e-6)\n",
    "    scores.append(smape(y[va], p))\n",
    "print(f\"CV SMAPE (word+char, Gamma GLM): {np.mean(scores):.2f}% ± {np.std(scores):.2f}%\")\n",
    "\n",
    "pipe_wc_gamma.fit(X_text_tr, y)\n",
    "pred_wc_gamma = pipe_wc_gamma.predict(X_text_te).clip(min=1e-6)\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": pred_wc_gamma}).to_csv(ART/\"submission_wc_gamma.csv\", index=False)\n",
    "print(\"Saved:\", (ART/\"submission_wc_gamma.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50a67c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_smape_opt.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cands = []\n",
    "for fname in [\"submission_wc.csv\", \"submission_wc_log.csv\", \"submission_wc_gamma.csv\"]:\n",
    "    p = (ART/fname)\n",
    "    if p.exists(): cands.append(pd.read_csv(p).rename(columns={\"price\": fname.replace(\".csv\",\"\")}))\n",
    "\n",
    "m = cands[0]\n",
    "for df in cands[1:]:\n",
    "    m = m.merge(df, on=\"sample_id\")\n",
    "\n",
    "# If both log and gamma exist, try a 60/40 toward the lower CV SMAPE\n",
    "cols = [c for c in m.columns if c!=\"sample_id\"]\n",
    "if \"submission_wc_log\" in cols and \"submission_wc_gamma\" in cols:\n",
    "    m[\"price\"] = 0.6*m[\"submission_wc_log\"] + 0.4*m[\"submission_wc_gamma\"]\n",
    "elif \"submission_wc_log\" in cols:\n",
    "    m[\"price\"] = m[\"submission_wc_log\"]\n",
    "else:\n",
    "    m[\"price\"] = m[cols[0]]\n",
    "\n",
    "out = ART/\"submission_wc_smape_opt.csv\"\n",
    "m[[\"sample_id\",\"price\"]].to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ac3c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bins = np.clip(np.floor(np.log1p(y)), 0, 10).astype(int)\n",
    "# If you want to keep KFold, keep the seed and ensure each fold has distribution checked\n",
    "# Or use StratifiedKFold on bins (regression hack):\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5a85f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE: D:\\amazon ML challenge\\artifacts\\test_out.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "# Make the final, portal-ready file from the log-target model\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "ART, DATA = Path(ART), Path(DATA)\n",
    "\n",
    "df = pd.read_csv(ART/\"submission_wc_log.csv\")[[\"sample_id\",\"price\"]]\n",
    "df[\"price\"] = df[\"price\"].astype(float).clip(lower=1e-6)\n",
    "\n",
    "# align to test.csv order (defensive)\n",
    "test_ids = pd.read_csv(DATA/\"test.csv\")[\"sample_id\"]\n",
    "final = test_ids.to_frame().merge(df, on=\"sample_id\", how=\"left\")\n",
    "assert final[\"price\"].notna().all()\n",
    "\n",
    "final.to_csv(ART/\"test_out.csv\", index=False)\n",
    "print(\"WROTE:\", (ART/\"test_out.csv\").resolve(), \"| rows:\", len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "367f2201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF SMAPE — log: 52.18425790496487 | tm: 66.27008136434968\n",
      "Best OOF SMAPE: 52.18% at w_log=1.00, w_tm=0.00\n",
      "Saved: D:\\amazon ML challenge\\artifacts\\submission_wc_smape_blend.csv\n"
     ]
    }
   ],
   "source": [
    "# === Build OOF predictions for two models, tune weights for SMAPE, blend test preds ===\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "ART, DATA = Path(ART), Path(DATA)\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-6):\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    denom = (np.abs(y_true)+np.abs(y_pred)+eps)/2.0\n",
    "    return float(np.mean(np.abs(y_true-y_pred)/denom)*100.0)\n",
    "\n",
    "# Rebuild the two models (same configs you used)\n",
    "identity = FunctionTransformer(lambda s: s, validate=False)\n",
    "\n",
    "# word+char features (same as Stage-2A)\n",
    "wc_features = Pipeline([\n",
    "    (\"union\", \n",
    "     Pipeline(steps=[(\"id\", identity)])),  # placeholder so we can set params next line\n",
    "])\n",
    "# We'll attach vectorizers directly via FeatureUnion-like manual mapping:\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "wc_features = FeatureUnion([\n",
    "    (\"word\", Pipeline([(\"id\", identity), (\"tfidf\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=350_000, min_df=2))])),\n",
    "    (\"char\", Pipeline([(\"id\", identity), (\"tfidf\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,6), max_features=300_000, min_df=2))]))\n",
    "], n_jobs=1)\n",
    "\n",
    "wc_log = Pipeline([\n",
    "    (\"features\", wc_features),\n",
    "    (\"ridge_log\", TransformedTargetRegressor(\n",
    "        regressor=Ridge(alpha=1.2, random_state=42),\n",
    "        func=np.log1p, inverse_func=np.expm1))\n",
    "])\n",
    "\n",
    "# word TF-IDF + meta (same as Stage-2B)\n",
    "numeric_cols = [\"num_count\",\"max_num\",\"min_num\",\"pack_n\",\"vol_ml\",\"wt_g\"]\n",
    "\n",
    "def extract_meta(s: pd.Series) -> pd.DataFrame:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "    nums = s.str.findall(r\"(?<![a-zA-Z])(\\d+(?:\\.\\d+)?)\")\n",
    "    num_count = nums.apply(len).astype(float)\n",
    "    max_num = nums.apply(lambda xs: max(map(float, xs)) if xs else np.nan).astype(float)\n",
    "    min_num = nums.apply(lambda xs: min(map(float, xs)) if xs else np.nan).astype(float)\n",
    "    pack = s.str.extract(r\"(?:pack of|pack)\\s*(\\d+)|(\\d+)\\s*-\\s*pack|(\\d+)\\s*pack\", expand=True)\n",
    "    pack_n = pack.apply(lambda row: next((int(x) for x in row if pd.notna(x)), np.nan), axis=1).astype(float)\n",
    "    uw = s.str.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(ml|l|oz|g|kg|lb)\")\n",
    "    def norm_units(pairs):\n",
    "        ml = g = None\n",
    "        for val, unit in pairs:\n",
    "            v = float(val)\n",
    "            if unit == \"ml\": ml = (ml or 0) + v\n",
    "            elif unit == \"l\":  ml = (ml or 0) + v*1000\n",
    "            elif unit == \"oz\": g  = (g  or 0) + v*28.3495\n",
    "            elif unit == \"g\":  g  = (g  or 0) + v\n",
    "            elif unit == \"kg\": g  = (g  or 0) + v*1000\n",
    "            elif unit == \"lb\": g  = (g  or 0) + v*453.592\n",
    "        return pd.Series({\"vol_ml\": ml if ml is not None else np.nan,\n",
    "                          \"wt_g\":  g  if g  is not None else np.nan})\n",
    "    unit_df = uw.apply(norm_units)\n",
    "    df = pd.DataFrame({\"num_count\": num_count, \"max_num\": max_num, \"min_num\": min_num, \"pack_n\": pack_n})\n",
    "    return pd.concat([df, unit_df], axis=1).fillna(0.0)\n",
    "\n",
    "train_text = X_text_tr\n",
    "test_text  = X_text_te\n",
    "y_vec = y\n",
    "\n",
    "meta_tr = extract_meta(train_text)\n",
    "meta_te = extract_meta(test_text)\n",
    "\n",
    "train_aug = pd.DataFrame({TEXT_COL: train_text})\n",
    "test_aug  = pd.DataFrame({TEXT_COL: test_text})\n",
    "for c in meta_tr.columns:\n",
    "    train_aug[c] = meta_tr[c]\n",
    "    test_aug[c]  = meta_te[c]\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"tfidf_word\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), max_features=300_000), TEXT_COL),\n",
    "    (\"num\", Pipeline([(\"scale\", StandardScaler(with_mean=False))]), numeric_cols),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "word_meta = Pipeline([(\"ct\", ct), (\"ridge\", Ridge(alpha=1.0, random_state=42))])\n",
    "\n",
    "# OOF predictions\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_log = np.zeros(len(train_aug))\n",
    "oof_tm  = np.zeros(len(train_aug))\n",
    "\n",
    "for tr_idx, va_idx in cv.split(train_aug):\n",
    "    wc_log.fit(train_text.iloc[tr_idx], y_vec[tr_idx])\n",
    "    oof_log[va_idx] = wc_log.predict(train_text.iloc[va_idx]).clip(min=1e-6)\n",
    "\n",
    "    word_meta.fit(train_aug.iloc[tr_idx], y_vec[tr_idx])\n",
    "    oof_tm[va_idx] = word_meta.predict(train_aug.iloc[va_idx]).clip(min=1e-6)\n",
    "\n",
    "print(\"OOF SMAPE — log:\", smape(y_vec, oof_log), \"| tm:\", smape(y_vec, oof_tm))\n",
    "\n",
    "# Grid search weights to minimize SMAPE on OOF\n",
    "best_w, best_s = None, 1e9\n",
    "for w in np.linspace(0, 1, 21):  # 0.00 .. 1.00 step 0.05\n",
    "    blend = w*oof_log + (1-w)*oof_tm\n",
    "    s = smape(y_vec, blend)\n",
    "    if s < best_s:\n",
    "        best_s, best_w = s, w\n",
    "print(f\"Best OOF SMAPE: {best_s:.2f}% at w_log={best_w:.2f}, w_tm={1-best_w:.2f}\")\n",
    "\n",
    "# Fit both on full train and blend test with tuned weights\n",
    "wc_log.fit(train_text, y_vec)\n",
    "p_log = wc_log.predict(test_text).clip(min=1e-6)\n",
    "\n",
    "word_meta.fit(train_aug, y_vec)\n",
    "p_tm = word_meta.predict(test_aug).clip(min=1e-6)\n",
    "\n",
    "p_blend = best_w*p_log + (1-best_w)*p_tm\n",
    "\n",
    "# Save SMAPE-optimized blend\n",
    "out = ART/\"submission_wc_smape_blend.csv\"\n",
    "pd.DataFrame({\"sample_id\": test[\"sample_id\"], \"price\": p_blend}).to_csv(out, index=False)\n",
    "print(\"Saved:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9f7a0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE: D:\\amazon ML challenge\\artifacts\\test_out.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ART, DATA = Path(ART), Path(DATA)\n",
    "best = \"submission_wc_smape_blend.csv\"  # or \"submission_wc_log.csv\" if you prefer\n",
    "\n",
    "df = pd.read_csv(ART/best)[[\"sample_id\",\"price\"]]\n",
    "df[\"price\"] = df[\"price\"].astype(float).clip(lower=1e-6)\n",
    "\n",
    "test_ids = pd.read_csv(DATA/\"test.csv\")[\"sample_id\"]\n",
    "final = test_ids.to_frame().merge(df, on=\"sample_id\", how=\"left\")\n",
    "assert final[\"price\"].notna().all()\n",
    "\n",
    "final.to_csv(ART/\"test_out.csv\", index=False)\n",
    "print(\"WROTE:\", (ART/\"test_out.csv\").resolve(), \"| rows:\", len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6e9869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE: D:\\amazon ML challenge\\artifacts\\test_out.csv | rows: 75000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "ART, DATA = Path(ART), Path(DATA)\n",
    "\n",
    "df = pd.read_csv(ART/\"submission_wc_log.csv\")[[\"sample_id\",\"price\"]]\n",
    "df[\"price\"] = df[\"price\"].astype(float).clip(lower=1e-6)\n",
    "\n",
    "test_ids = pd.read_csv(DATA/\"test.csv\")[\"sample_id\"]\n",
    "final = test_ids.to_frame().merge(df, on=\"sample_id\", how=\"left\")\n",
    "assert final[\"price\"].notna().all(), \"Missing predictions\"\n",
    "\n",
    "final.to_csv(ART/\"test_out.csv\", index=False)\n",
    "print(\"WROTE:\", (ART/\"test_out.csv\").resolve(), \"| rows:\", len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25c25d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SMAPE (word+char ridge): 68.12% ± 0.19%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    # Avoid divisions by 0 (when both true and pred are 0)\n",
    "    mask = denom != 0\n",
    "    out = np.zeros_like(denom)\n",
    "    out[mask] = diff[mask] / denom[mask]\n",
    "    return np.mean(out) * 100.0\n",
    "\n",
    "# Example: evaluate smape for the word+char pipeline 'pipe_wc'\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "for tr, va in cv.split(X_text_tr):\n",
    "    pipe_wc.fit(X_text_tr.iloc[tr], y[tr])\n",
    "    p = pipe_wc.predict(X_text_tr.iloc[va])\n",
    "    scores.append(smape(y[va], p))\n",
    "print(f\"CV SMAPE (word+char ridge): {np.mean(scores):.2f}% ± {np.std(scores):.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
